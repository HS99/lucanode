# Discussion
\newpage

## Lung segmentation
The initial approach we wanted to follow for the lung segmentation was to create a multiatlas lung segmentation such as the ones described in \[@Rohlfing2004, @VanRikxoort2009a\]. Still, due to the required computational time (registration libraries only operate on CPU) and the fact that lung segmentation has a relative low impact in the final score of the system, we chose to employ the same architecture we had been using to segment nodules for the lungs themselves.

Perhaps unsurprising, but still very interesting, to demonstrate the high degree of transferability of the network architecture to different tasks, just by adjusting the input masks. Even the issues the network was currently facing segmenting the lower lobule could very likely be fixed by rebalancing the dataset to oversample that part of the lung. Also, there are some other easily corrected errors, such as the removal of holes inside masks. Even after applying these fixes, we could still bring things further by applying some image preprocessing to increase the contrast of the image, or applying augmentation to safely oversample troublesome slices.

## Nodule detection
The first thing we would have to do with the current results is to visualize the currenly undetected nodules and try to understand what similarities do they share. If they are mostly non-solid nodules, we could try to find another image filter that highly increases their contrast. If that does not help, it might be time to consider using a 3D U-Net architecture, or maybe an object detection network (based on an image recognition model) instead of a segmentation one. The intuition behind this affirmation, apart from our review of the state of the art, is also based in the resulting masks for the false positive candidates. They tend to be too flat, which is a sign that the network is not capturing depth properly. In this case it might also be worth it to train another 2D segmentation layer over another one of the CT planes (i.e. sagittal). Once that would be ready, we could use both models to create an ensemble that better captures the shape of a nodule.

This section of the thesis has also been a very good practical demonstration on how small details can make or break a network's performance. For a long time, we were trying to train the U-Net without a batch normalization layer, which didn't converge. That was one of the multiple times where the non-linear progress on the development of this project came into full display. Also, related to the batch normalization, there are other strategies, like the one presented in \[@Wu2018\] which look promising for our case at hand, since the length of the batch is severily limited due to the size of the images.

## False positive reduction
The classifier based on handpicked features was an interesting use case of the remarkably low number of false positives returned by our U-Net model. It was only thanks to the strength of the original segmentation network that such approach could ever prove useful. Still, an approach like this is not without drawbacks. It simplifies the system, but it introduced dependencies between layers. Can we affirm that the same model would equally work if the underlying U-Net had been trained with another loss function? Or maybe another set of masks? It probably wouldn't, since the results are very much dependent on the original network. This feedback also introduces some questions regarding the generalization of the model (performance between training and testing suffers a noticeable drop, especially compared to the model trained with residual networks). Finally, the fact that we are relying on the output of the segmentation network also limits the number of examples we have to train the classifier, which caps its performance. The usage of image augmentation might have been a worthwhile idea to explore, although this would involve quite a bit of image preprocessing. Probably not enough to justify the improvement in performance.

To our surprise, the classifier based on automatically extracted radiomic features actually performed worse than our manually handpicked classifier. A possible explanation to this fact could be that the segmentation itself we're parting from is already far from a proper representation of a nodule, and this handicaps its performance (radiomics are usually applied on manual segmentation of tumors to characterize them, not to classify lesions from non-lesions). It is also interesting noting that a PCA with lesser components is able to outperform the original 105 features. This suggests that the scattershot approach of radiomics might not actually deliver on its promises. Still, we would like to hold our judgment on the technique unless applied on a dataset with proper segmentations and maybe another kind of classification task. For example, applying radiomics to predict malignancy from a set of nodule segmentations would probably provide a fairer appraisal of the technique.

Finally, we prove again why image recognition methods based on deep learning are the current state of the art. A volumetric convolutional network, based on a residual architecture, beats previous systems by more than a 10% margin in the final LUNA FROC score. Even though the ResNet beats the other systems, we still face the problem that the deeper layered versions saturate (probably due to the small input size of the image). For this particular experiment we've also had to face a few technical limitations that prevented us from fully utilizing the whole available dataset, which surely didn't help on the final score.

In an improved version of the system, it would be interesting to explore the impact of using multiple ResNets trained on multiple input fields. Having multiple models trained at different scales would be best to capture the inherent nodule heterogeneity. If we only use one model we have to ensure that the size of the input image is going to be big enough to capture the vast majority of nodules inside it, which might be too big a cube for the majority of nodules (below 8.5mm in diameter). Also relevant for the false positive reduction track, we are using an isotropic resize to 1x1x1mm, sacrificing resolution in the axial plane (originally around 0.6mm). If the resize had been performed at 0.6mm or 0.5mm for this particular part of the pipeline, we would also have more information from the original image, which might positively impact in the overall performance of the system.


## LUNA performance comparative
We have already commented on possible improvements on both nodule detection and false positive reduction in the previous two sections, so we are not going to cover the topic. Instead, we would like to focus on the importance of the FROC metrics to determine where the development efforts are best spent.

Compared to our immediate competitors, we have a competitive upper bound but the FROC curve is not as flat. This readily suggests that our false positive reduction system is not as competitive and should be the first component of the system to be brought for assessment. This is just an example of how visualizations are key into gaining insights about the system, which has been a constant throughout the project.

It is also interesting to note how small gains in the ranking actually require an exponential amount of work. We can achieve top-20 results by employing a single U-Net model and engineering a few features from the resulting segmentations, but we require training a deep residual network for days to improve the ranking by 2 spots. Going further than that would surely demand longer training times, a more creative use of image preprocessing, fully switching to volumetric models and embracing ensembling, which would force us to deal with the vicissitudes of each one of its models, which just keeps raising the bar for such a system to be ever deployed in production.

## Integration into a clinical workflow
One of the questions we haven't directly adressed in this thesis is how *lucanode* could be integrated into a clinical workflow. Ideally a CAD such as this one would be part of the hospital protocol, and automatically executed whenever a thoracic CT scan is performed. This would always provide a second opinion to the radiologist in charge of diagnosing the results and, hopefully, result in a lower *interobserver variability*, that is, the performance gap between an experienced radiologist and a resident, for example.

Usually systems participating in challenges such as LUNA are only worried about the raw performance of their algorithm, which puts the possibility of executing it end to end as part of an integrated pipeline very far down the list of priorities. We didn't want to do this with lucanode, so we took steps towards making the results easier to reproduce and execute on an arbitrary scan.

The first step towards integrating the pipeline is ensuring the preprocessing of the scans can be performed on the fly. This is the reason why we've trained an automated lung segmentation model. Also, as part of this model, we're also performing the CT scan voxel resampling, so all the necessary inputs to execute the nodule segmentation and false positive reduction are in place.

Another important concern that needs to be tackled is the packaging of the software. A self-contained solution is much easier to setup and distribute, but it takes effort to prepare such a package. In our case, thanks to conda and Docker, we have readily available images that contain both the code and the model weights to execute the *lucanode* system as if it were a self-contained binary. There is even experimental support for nvidia docker \[@NVIDIA2018\], a containerization technology capable of using the underlying GPU resources of its host. In fact, even though we haven't properly benchmarked the system for this thesis, the results of our effort are already live and have been presented in Albert Moral's graduation final project \[@MoralLleo2018\].

For a future revision of the work, we would like to properly benchmark the system and also take advantage of it by analyzing its performance in other datasets than LUNA. This would also prove to be a very valuable addition to the literature, as we could check how well the model generalizes to different datasets.

## Future work
As far as improving the performance of the current system, the logical step forward is to create an ensemble of multiple models to improve its overall performance. There are also other paths worth pursuing. For example, the candidate detection network could be changed from a segmentation network into an object detection one. This way we could use the same model we employ for false positive reduction to detect the nodules themselves. It would be interesting to test the usage of a R-CNN \[@Ren2017\] or a YOLO \[@Ren2017\]. Plus, we are not aware of such architectures having been used to detect objects in a volume, so that could be a worthy contribution in and of itself.

Apart from incremental improvements on the performance of the current system, there is an angle which hasn't been explored, and that is inferring the malignancy of the nodules. The LUNA dataset does not contain such information, but the dataset it is based on (LIDC-IDRI) contains annotations by radiologists with the estimated malignancy per nodule, a real handmade segmentation, and even the real diagnostic for a few of them. All this information could be used to transform the false positive reduction system into a malignancy predictor, so the system would not only detect abnormalities, it would also diagnose them.

## General discussion
The most unexpected side effect of developing *lucanode* has been the paradigm shift from traditional software development, where you are iteratively coding an increasing set of rules to make the system smarter to training models, where you are repackaging your data the way your network understands it best. In a way, training a deep learning model is just about finding the metaphor that makes it click. Data scientists, like comedians, need to find the right kind of humour for their audience.

I come from an engineering background and I am more used to blacks and whites than shades of grey, so the change has been quite a shock. On the one hand, the performance of such systems is first-class, but they also make you wonder about their maintainability in the long term. How can you fix what you can't explain? In fact, you can check \[@Sculley2014\] for more on the topic.

Another aspect I would like to mention is the brittleness of these models, especially on systems with pipelines that feed one another. Any error, anywhere in the pipeline, can invalidate the whole experiment. And it is very typical to have them, especially since there are shared elements that need to be setup in slightly different, but very significant, ways. It gets especially bad when you want to train and evaluate different variations of the same model side by side. You need to do it to properly test your hypotheses, but that in itself increases the error rate of said experiment.

My best recommendation to the grievings above is to work on the visualizations. Whenever I've been stuck, whenever I wanted to confirm a result, I've always found solace in a good visualization. They are the best way to confirm your system works as intended and the best way to understand where it doesn't.

