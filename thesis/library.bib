Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Siegel2018,
author = {Siegel, Rebecca L and Miller, Kimberly D and Jemal, Ahmedin},
doi = {10.3322/caac.21442},
file = {:Users/octavi/Documents/Mendeley Desktop/Siegel{\_}et{\_}al-2018-CA{\%}3A{\_}A{\_}Cancer{\_}Journal{\_}for{\_}Clinicians.pdf:pdf},
keywords = {cancer cases,cancer statistics,death rates,incidence,mortality},
number = {1},
pages = {7--30},
title = {{Cancer Statistics , 2018}},
volume = {68},
year = {2018}
}
@misc{Kanalley,
author = {Kanalley, Craig},
file = {:Users/octavi/Documents/Mendeley Desktop/Kanalley - 2013 - Why Facebook Cares So Much About Mobile, Seeks to Be 'Mobile Best'.html:html},
keywords = {'mobile,about,be,best',cares,facebook,mobile,much,seeks,so,technology,to,why},
title = {{Why Facebook Cares So Much About Mobile, Seeks to Be 'Mobile Best'}},
url = {http://www.huffingtonpost.com/craig-kanalley/facebook-mobile{\_}b{\_}2994631.html},
urldate = {2013-06-14},
year = {2013}
}
@article{Cootes1998,
abstract = {We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors},
archivePrefix = {arXiv},
arxivId = {10.1.1.128.4967},
author = {Cootes, T.F. and Edwards, G.J. and Taylor, C.J.},
doi = {10.1109/34.927467},
eprint = {10.1.1.128.4967},
file = {:Users/octavi/Documents/Mendeley Desktop/Active appearence models.pdf:pdf},
isbn = {3540646132},
issn = {0162-8828},
journal = {Proc. European Conference on Computer Vision (ICCV)},
pages = {484--498},
pmid = {927467},
title = {{Active Appearance Models}},
volume = {2},
year = {1998}
}
@book{Wilder2012,
author = {Wilder, Bill},
file = {:Users/octavi/Documents/Mendeley Desktop/Wilder - 2012 - Cloud Architecture Patterns.pdf:pdf},
isbn = {9781449319779},
pages = {182},
publisher = {O'Reilly},
title = {{Cloud Architecture Patterns}},
year = {2012}
}
@article{Gillies2016,
abstract = {radiomics In the past decade, the field of medical image analysis has grown exponentially, with an increased number of pattern recognition tools and an increase in data set sizes. These advances have facilitated the development of processes for high-throughput extraction of quantitative features that result in the conversion of images into mineable data and the subsequent analysis of these data for decision support; this practice is termed radiomics. This is in contrast to the traditional practice of treating medical images as pictures intended solely for visual interpretation. Radiomic data contain first-, second-, and higher-order statistics. These data are combined with other patient data and are mined with sophisticated bioinformatics tools to develop models that may potentially improve diagnostic, prognostic, and predictive accuracy. Because radiomics analyses are intended to be conducted with standard of care images, it is conceivable that conversion of digital images to mineable data will eventually become routine practice. This report describes the process of radiomics, its challenges, and its potential power to facilitate better clinical decision making, particularly in the care of patients with cancer.},
author = {Gillies, Robert J. and Kinahan, Paul E. and Hricak, Hedvig},
doi = {10.1148/radiol.2015151169},
file = {:Users/octavi/Documents/Mendeley Desktop/radiol.2015151169.pdf:pdf},
isbn = {0033-8419},
issn = {0033-8419},
journal = {Radiology},
number = {2},
pages = {563--577},
pmid = {26579733},
title = {{Radiomics: Images Are More than Pictures, They Are Data}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2015151169},
volume = {278},
year = {2016}
}
@article{Symposium2010,
author = {Symposium, Biennial},
file = {:Users/octavi/Documents/Mendeley Desktop/Symposium - 2010 - 25th Biennial Symposium on Communications.pdf:pdf},
isbn = {9781424457113},
keywords = {evaluation,fairness,tcp},
pages = {80--83},
title = {{25th Biennial Symposium on Communications}},
year = {2010}
}
@misc{VanGriethuysen2018,
author = {van Griethuysen, Joost J.M.},
title = {{PyRadiomics documentation - features}},
url = {http://pyradiomics.readthedocs.io/en/latest/features.html},
year = {2018}
}
@article{NationalKidneyFoundation2002,
abstract = {These guidelines are based upon the best information available at the time of publica- tion. They are designed to provide information and assist decision making. They are not intended to define a standard of care, and should not be construed as doing so. Neither should they be interpreted as prescribing an exclusive course of management. Variations in practice will inevitably and appropriately occur when clinicians take into account the needs of individual patients, available resources, and limitations unique to an institution or type of practice. Every healthcare professional making use of these guidelines is responsible for evaluating the appropriateness of applying them in the setting of any particular clinical situation. The recommendations for research contained within this document are general and not meant to imply a specific protocol.},
author = {{National Kidney Foundation}},
doi = {10.1634/theoncologist.2011-S2-45},
file = {:Users/octavi/Documents/Mendeley Desktop/Unknown - Unknown - CLINICAL PRACTICE GUIDELINES.pdf:pdf},
isbn = {1-931472-10-6},
issn = {1541-7891},
journal = {Am J Kidney Dis.},
keywords = {IRC},
number = {(suppl. 1)},
pages = {1--266},
pmid = {21093403},
title = {{K/DOQI Clinical Practice Guidelines for Chronic Kidney Disease: Evaluation, Clasification and Stratification}},
url = {https://www.kidney.org/sites/default/files/docs/ckd{\_}evaluation{\_}classification{\_}stratification.pdf www.kdoqi.org},
volume = {39},
year = {2002}
}
@article{Kauczor2015,
abstract = {Lung cancer is the most frequently fatal cancer, with poor survival once the disease is advanced. Annual low-dose computed tomography has shown a survival benefit in screening individuals at high risk for lung cancer. Based on the available evidence, the European Society of Radiology and the European Respiratory Society recommend lung cancer screening in comprehensive, quality-assured, longitudinal programmes within a clinical trial or in routine clinical practice at certified multidisciplinary medical centres. Minimum requirements include: standardised operating procedures for low-dose image acquisition, computer-assisted nodule evaluation, and positive screening results and their management; inclusion/exclusion criteria; expectation management; and smoking cessation programmes. Further refinements are recommended to increase quality, outcome and cost-effectiveness of lung cancer screening: inclusion of risk models, reduction of effective radiation dose, computer-assisted volumetric measurements and assessment of comorbidities (chronic obstructive pulmonary disease and vascular calcification). All these requirements should be adjusted to the regional infrastructure and healthcare system, in order to exactly define eligibility using a risk model, nodule management and a quality assurance plan. The establishment of a central registry, including a biobank and an image bank, and preferably on a European level, is strongly encouraged. KEY POINTS: • Lung cancer screening using low dose computed tomography reduces mortality. • Leading US medical societies recommend large scale screening for high-risk individuals. • There are no lung cancer screening recommendations or reimbursed screening programmes in Europe as of yet. • The European Society of Radiology and the European Respiratory Society recommend lung cancer screening within a clinical trial or in routine clinical practice at certified multidisciplinary medical centres. • High risk, eligible individuals should be enrolled in comprehensive, quality-controlled longitudinal programmes.},
author = {Kauczor, Hans-Ulrich and Bonomo, Lorenzo and Gaga, Mina and Nackaerts, Kristiaan and Peled, Nir and Prokop, Mathias and Remy-Jardin, Martine and von Stackelberg, Oyunbileg and Sculier, Jean-Paul},
doi = {10.1007/s00330-015-3697-0},
file = {:Users/octavi/Documents/Mendeley Desktop/330{\_}2015{\_}Article{\_}3697.pdf:pdf},
isbn = {0903-1936},
issn = {0938-7994},
journal = {European Radiology},
number = {9},
pages = {2519--2531},
pmid = {25929939},
title = {{ESR/ERS white paper on lung cancer screening}},
url = {http://link.springer.com/10.1007/s00330-015-3697-0},
volume = {25},
year = {2015}
}
@book{David2007,
author = {David, Michele E. and Phillips, Jon A.},
edition = {2nd},
file = {:Users/octavi/Documents/Mendeley Desktop/David, Phillips - 2007 - Learn PHP and MySQL.pdf:pdf},
isbn = {9780596514013},
pages = {430},
publisher = {O'Reilly},
title = {{Learn PHP and MySQL}},
year = {2007}
}
@misc{Belshec,
author = {Belshe, Mike},
file = {:Users/octavi/Documents/Mendeley Desktop/Belshe - 2010 - More Bandwidth Doesn't Matter (Much).html:html},
title = {{More Bandwidth Doesn't Matter (Much)}},
url = {https://docs.google.com/a/chromium.org/viewer?a=v{\&}pid=sites{\&}srcid=Y2hyb21pdW0ub3JnfGRldnxneDoxMzcyOWI1N2I4YzI3NzE2},
urldate = {2013-06-14},
year = {2010}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:Users/octavi/Documents/Mendeley Desktop/1505.04597.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
pages = {1--8},
pmid = {23285570},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{Nielsen2016,
author = {Nielsen, Didrik},
file = {:Users/octavi/Documents/Mendeley Desktop/why{\_}xgboost{\_}wins.pdf:pdf},
journal = {NTNU Tech Report},
number = {December},
pages = {2016},
title = {{Tree Boosting With XGBoost Why Does XGBoost Win "Every" Machine Learning Competition?}},
url = {https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128{\_}FULLTEXT.pdf?sequence=1{\&}isAllowed=y},
year = {2016}
}
@misc{Nia,
annote = {Summary: There are 3 main time limits (which are determined by human perceptual abilities) to keep in mind when optimizing web and application performance.},
author = {Nielsen, Jakob},
file = {:Users/octavi/Documents/Mendeley Desktop/Nielsen - 1993 - Response Time Limits.html:html},
title = {{Response Time Limits}},
url = {http://www.nngroup.com/articles/response-times-3-important-limits/},
urldate = {2013-05-18},
year = {1993}
}
@article{Chang2008,
abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.},
author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E},
doi = {10.1145/1365815.1365816},
institution = {Google, Inc.},
isbn = {1931971471},
issn = {07342071},
journal = {Sports Illustrated},
number = {2},
pages = {1--26},
publisher = {ACM},
series = {OSDI '06},
title = {{Bigtable : A Distributed Storage System for Structured Data}},
url = {http://portal.acm.org/citation.cfm?id=1365816},
volume = {26},
year = {2008}
}
@article{Wu2018,
abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6{\%} lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform or compete with its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
archivePrefix = {arXiv},
arxivId = {1803.08494},
author = {Wu, Yuxin and He, Kaiming},
eprint = {1803.08494},
file = {:Users/octavi/Documents/Mendeley Desktop/1803.08494.pdf:pdf},
title = {{Group Normalization}},
url = {http://arxiv.org/abs/1803.08494},
year = {2018}
}
@article{Heckemann2006,
abstract = {Regions in three-dimensional magnetic resonance (MR) brain images can be classified using protocols for manually segmenting and labeling structures. For large cohorts, time and expertise requirements make this approach impractical. To achieve automation, an individual segmentation can be propagated to another individual using an anatomical correspondence estimate relating the atlas image to the target image. The accuracy of the resulting target labeling has been limited but can potentially be improved by combining multiple segmentations using decision fusion. We studied segmentation propagation and decision fusion on 30 normal brain MR images, which had been manually segmented into 67 structures. Correspondence estimates were established by nonrigid registration using free-form deformations. Both direct label propagation and an indirect approach were tested. Individual propagations showed an average similarity index (SI) of 0.754 ± 0.016 against manual segmentations. Decision fusion using 29 input segmentations increased SI to 0.836 ± 0.009. For indirect propagation of a single source via 27 intermediate images, SI was 0.779 ± 0.013. We also studied the effect of the decision fusion procedure using a numerical simulation with synthetic input data. The results helped to formulate a model that predicts the quality improvement of fused brain segmentations based on the number of individual propagated segmentations combined. We demonstrate a practicable procedure that exceeds the accuracy of previous automatic methods and can compete with manual delineations. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Heckemann, Rolf A. and Hajnal, Joseph V. and Aljabar, Paul and Rueckert, Daniel and Hammers, Alexander},
doi = {10.1016/j.neuroimage.2006.05.061},
file = {:Users/octavi/Documents/Mendeley Desktop/heckemann2006.pdf:pdf},
isbn = {1053-8119 (Print)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
number = {1},
pages = {115--126},
pmid = {16860573},
title = {{Automatic anatomical brain MRI segmentation combining label propagation and decision fusion}},
volume = {33},
year = {2006}
}
@article{Armbrust2009,
abstract = {Provided certain obstacles are overcome, we believe Cloud Computing has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new interactive Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about over-provisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or under-provisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get their results as quickly as their programs can scale, since using 1000 servers for one hour costs no more than using one server for 1000 hours. This elasticity of resources, without paying a premium for large scale, is unprecedented in the history of IT. The economies of scale of very large-scale datacenters combined with ``pay-as-you-go'' resource usage has heralded the rise of Cloud Computing. It is now attractive to deploy an innovative new Internet service on a third party's Internet Datacenter rather than your own infrastructure, and to gracefully scale its resources as it grows or declines in popularity and revenue. Expanding and shrinking daily in response to normal diurnal patterns could lower costs even further. Cloud Computing transfers the risks of over-provisioning or under-provisioning to the Cloud Computing provider, who mitigates that risk by statistical multiplexing over a much larger set of users and who offers relatively low prices due better utilization and from the economy of purchasing at a larger scale. We define terms, present an economic model that quantifies the key buy vs. pay-as-you-go decision, offer a spectrum to classify Cloud Computing providers, and give our view of the top 10 obstacles and opportunities to the growth of Cloud Computing.},
author = {Armbrust, Michael and Joseph, Anthony D and Katz, Randy H and Patterson, David A},
doi = {10.1145/1721654.1721672},
file = {:Users/octavi/Documents/Mendeley Desktop/Armbrust et al. - 2009 - Above the Clouds A Berkeley View of Cloud Computing.pdf:pdf},
institution = {EECS Department, University of California, Berkeley},
issn = {00010782},
journal = {Science},
number = {UCB/EECS-2009-28},
pages = {07--013},
publisher = {Citeseer},
series = {UCB/EECS-2009-28},
title = {{Above the Clouds : A Berkeley View of Cloud Computing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.7163{\&}rep=rep1{\&}type=pdf},
volume = {53},
year = {2009}
}
@article{Muller-Putz2015b,
abstract = {The main objective of this roadmap is to provide a global perspective on the BCI field now and in the future. For readers not familiar with BCIs, we introduce basic terminology and concepts. We discuss what BCIs are, what BCIs can do, and who can benefit from BCIs. We illustrate our arguments with use cases to support the main messages. After reading this roadmap you will have a clear picture of the potential benefits and challenges of BCIs, the steps necessary to bridge the gap between current and future applications, and the potential impact of BCIs on society in the next decade and beyond.},
author = {Muller-Putz, G. R. and Brunner, Clemens and Bauernfeind, G. and Blefari, M. L. and Mill{\'{a}}n, Jos{\'{e}} del R. and Real, R. G. L. and K{\"{u}}bler, Andrea and Mattia, Donatella and Pichiorri, F. and Schettini, F. and Ramsey, Nick and H{\"{o}}hne, J. and Blankertz, Benjamin and Miralles, Felip and Otal, B. and Guger, Christoph and Ortner, Rupert and Poel, Mannes and Nijholt, Anton and Reuderink, Boris and Birbaumer, Niels and de Pobes, A. and Salomon, Patric and van Steensel, M. and Soekader, S. and Opisso, E.},
doi = {10.3390/s140814601.Allison},
file = {:Users/octavi/Documents/Mendeley Desktop/Appendix{\_}B{\_}Industry.pdf:pdf},
isbn = {9783851253795},
keywords = {Brain-Computer Interfaces},
pages = {1--48},
title = {{BNCI Horizon 2020 The Future of Brain / Neural Computer Interaction : Horizon 2020 Appendix A Research}},
url = {http://dx.doi.org/10.3217/978-3-85125-379-5{\%}5Cnhttp://bnci-horizon-2020.eu/images/bncih2020/Appendix{\_}A{\_}Research.pdf},
volume = {2020},
year = {2015}
}
@article{Amiri2013,
author = {Amiri, Setare and Fazel-rezai, Reza and Asadpour, Vahid},
doi = {10.1155/2013/187024},
file = {:Users/octavi/Documents/Mendeley Desktop/p1-amiri.pdf:pdf},
issn = {1687-5893},
title = {{Review Article A Review of Hybrid Brain-Computer Interface Systems}},
volume = {2013},
year = {2013}
}
@misc{Souders,
author = {Souders, Steve},
file = {:Users/octavi/Documents/Mendeley Desktop/Souders - 2012 - Best of Fluent 2012 - High Performance Snippets.html:html},
title = {{Best of Fluent 2012 - High Performance Snippets}},
url = {http://www.youtube.com/watch?v=ZPmotBbYw-w},
urldate = {2013-06-15},
year = {2012}
}
@article{Rajkomar2018,
abstract = {Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient's record. We propose a representation of patients' entire, raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two U.S. academic medical centers with 216,221 adult patients hospitalized for at least 24 hours. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting in-hospital mortality (AUROC across sites 0.93-0.94), 30-day unplanned readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed state-of-the-art traditional predictive models in all cases. We also present a case-study of a neural-network attribution system, which illustrates how clinicians can gain some transparency into the predictions. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios, complete with explanations that directly highlight evidence in the patient's chart.},
archivePrefix = {arXiv},
arxivId = {1801.07860},
author = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M. and Hajaj, Nissan and Liu, Peter J. and Liu, Xiaobing and Sun, Mimi and Sundberg, Patrik and Yee, Hector and Zhang, Kun and Duggan, Gavin E. and Flores, Gerardo and Hardt, Michaela and Irvine, Jamie and Le, Quoc and Litsch, Kurt and Marcus, Jake and Mossin, Alexander and Tansuwan, Justin and Wang, De and Wexler, James and Wilson, Jimbo and Ludwig, Dana and Volchenboum, Samuel L. and Chou, Katherine and Pearson, Michael and Madabushi, Srinivasan and Shah, Nigam H. and Butte, Atul J. and Howell, Michael and Cui, Claire and Corrado, Greg and Dean, Jeff},
eprint = {1801.07860},
file = {:Users/octavi/Documents/Mendeley Desktop/1801.07860.pdf:pdf},
pages = {1--26},
title = {{Scalable and accurate deep learning for electronic health records}},
url = {http://arxiv.org/abs/1801.07860},
year = {2018}
}
@article{Fomina2015,
author = {Fomina, Tatiana and Sch, Bernhard},
file = {:Users/octavi/Documents/Mendeley Desktop/07332703.pdf:pdf},
isbn = {9781467394819},
journal = {7th Computer Science and Electronic Engineering Conference},
pages = {77--80},
title = {{Towards Cognitive Brain-Computer Interfaces for Patients with Amyotrophic Lateral Sclerosis}},
year = {2015}
}
@article{Huang2017,
abstract = {Computer-aided diagnosis significantly reduces the low-dose CT screening false-positive rate and increases the positive predictive value of lung nodule evaluation.},
author = {Huang, Peng and Park, Seyoun and Yan, Rongkai and Lee, Junghoon and Chu, Linda C. and Lin, Cheng T. and Hussien, Amira and Rathmell, Joshua and Thomas, Brett and Chen, Chen and Hales, Russell and Ettinger, David S. and Brock, Malcolm and Hu, Ping and Fishman, Elliot K. and Gabrielson, Edward and Lam, Stephen},
doi = {10.1148/radiol.2017162725},
file = {:Users/octavi/Documents/Mendeley Desktop/huang2017.pdf:pdf},
issn = {0033-8419},
journal = {Radiology},
number = {0},
pages = {162725},
pmid = {28872442},
title = {{Added Value of Computer-aided CT Image Features for Early Lung Cancer Diagnosis with Small Pulmonary Nodules: A Matched Case-Control Study}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2017162725},
volume = {000},
year = {2017}
}
@article{Dou2017,
abstract = {False positive reduction is one of the most crucial components in an automated pulmonary nodule detection system, which plays an important role in lung cancer diagnosis and early treatment. The objective of this paper is to effectively address the challenges in this task and therefore to accurately discriminate the true nodules from a large number of candidates. METHODS We propose a novel method employing 3D convolutional neural networks (CNNs) for false positive reduction in automated pulmonary nodule detection from volumetric CT scans. Compared with its 2D counterparts, the 3D CNNs can encode richer spatial information and extract more representative features via their hierarchical architecture trained with 3D samples. More importantly, we further propose a simple yet effective strategy to encode multi-level contextual information to meet the challenges coming with the large variations and hard mimics of pulmonary nodules. RESULTS The proposed framework has been extensively validated in the LUNA16 challenge held in conjunction with ISBI 2016, where we achieved the highest competition performance metric (CPM) score in the false positive reduction track. CONCLUSION Experimental results demonstrated the importance and effectiveness of integrating multi-level contextual information into 3D CNN framework for automated pulmonary nodule detection in volumetric CT data. SIGNIFICANCE While our method is tailored for pulmonary nodule detection, the proposed framework is general and can be easily extended to many other 3D object detection tasks from volumetric medical images, where the targeting objects have large variations and are accompanied by a number of hard mimics.},
author = {Dou, Qi and Chen, Hao and Yu, Lequan and Qin, Jing and Heng, Pheng Ann},
doi = {10.1109/TBME.2016.2613502},
file = {:Users/octavi/Documents/Mendeley Desktop/Dou et al. - 2017 - Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection.pdf:pdf},
isbn = {2013101015},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {3-D convolutional neural networks,Computer-aided diagnosis,deep learning,false positive reduction,pulmonary nodule detection},
pmid = {28113302},
title = {{Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection}},
year = {2017}
}
@article{Adomavicius2005,
abstract = {This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.},
archivePrefix = {arXiv},
arxivId = {3},
author = {Adomavicius, G and Tuzhilin, a},
doi = {10.1109/TKDE.2005.99},
eprint = {3},
file = {:Users/octavi/Documents/Mendeley Desktop/recommender-systems-survey-2005.pdf:pdf},
isbn = {1066100802},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {collaborative filtering,extensions recommender systems.,index terms recommender systems,rating estimation methods},
number = {6},
pages = {734--749},
pmid = {1423975},
title = {{Toward the Next Generation of Recommender Systems: a Survey of the State of the Art and Possible Extensions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1423975},
volume = {17},
year = {2005}
}
@article{VanRikxoort2009a,
abstract = {Lung segmentation is a prerequisite for automated analysis of chest CT scans. Conventional lung segmentation methods rely on large attenuation differences between lung parenchyma and surrounding tissue. These methods fail in scans where dense abnormalities are present, which often occurs in clinical data. Some methods to handle these situations have been proposed, but they are too time consuming or too specialized to be used in clinical practice. In this article, a new hybrid lung segmentation method is presented that automatically detects failures of a conventional algorithm and, when needed, resorts to a more complex algorithm, which is expected to produce better results in abnormal cases. In a large quantitative evaluation on a database of 150 scans from different sources, the hybrid method is shown to perform substantially better than a conventional approach at a relatively low increase in computational cost.},
author = {{Van Rikxoort}, Eva M. and {De Hoop}, Bartjan and Viergever, Max A. and Prokop, Mathias and {Van Ginneken}, Bram},
doi = {10.1118/1.3147146},
file = {:Users/octavi/Documents/Mendeley Desktop/10.1118@1.3147146.pdf:pdf},
isbn = {0094-2405 (Print)$\backslash$n0094-2405 (Linking)},
issn = {00942405},
journal = {Medical Physics},
keywords = {Automatic,CT,Error detection,Lung,Segmentation},
number = {7},
pages = {2934--2947},
pmid = {19673192},
title = {{Automatic lung segmentation from thoracic computed tomography scans using a hybrid approach with error detection}},
volume = {36},
year = {2009}
}
@misc{Berners-Lee1996,
author = {Berners-Lee, T and MIT/LCS and Fielding, R and {UC Irvine} and Frystyk, H},
file = {:Users/octavi/Documents/Mendeley Desktop/Berners-Lee et al. - 1996 - Hypertext Transfer Protocol -- HTTP1.0 -- RFC1945.html:html},
title = {{Hypertext Transfer Protocol -- HTTP/1.0 -- RFC1945}},
url = {http://www.w3.org/Protocols/rfc1945/rfc1945},
urldate = {2013-06-14},
year = {1996}
}
@article{Ohm2012,
author = {Ohm, Jens-rainer and Sullivan, Gary J and Schwarz, Heiko and Tan, Thiow Keng and Member, Senior and Wiegand, Thomas},
file = {:Users/octavi/Documents/Mendeley Desktop/Ohm et al. - 2012 - Comparison of the Coding Efficiency of Video Coding Standards — Including High Efficiency Video Coding ( HEVC ).pdf:pdf},
number = {12},
pages = {1669--1684},
title = {{Comparison of the Coding Efficiency of Video Coding Standards — Including High Efficiency Video Coding ( HEVC )}},
volume = {22},
year = {2012}
}
@article{Steeg2016,
abstract = {Tumour metastasis, the movement of tumour cells from a primary site to progressively colonize distant organs, is a major contributor to the deaths of cancer patients. Therapeutic goals are the prevention of an initial metastasis in high-risk patients, shrinkage of established lesions and prevention of additional metastases in patients with limited disease. Instead of being autonomous, tumour cells engage in bidirectional interactions with metastatic microenvironments to alter antitumour immunity, the extracellular milieu, genomic stability, survival signalling, chemotherapeutic resistance and proliferative cycles. Can targeting of these interactions significantly improve patient outcomes? In this Review preclinical research, combination therapies and clinical trial designs are re-examined.},
author = {Steeg, Patricia S.},
doi = {10.1038/nrc.2016.25},
file = {:Users/octavi/Documents/Mendeley Desktop/taregting{\_}metastasis.pdf:pdf},
isbn = {1474-1768 (Electronic) 1474-175X (Linking)},
issn = {1474-175X},
journal = {Nature Reviews Cancer},
number = {4},
pages = {201--218},
pmid = {27009393},
publisher = {Nature Publishing Group},
title = {{Targeting metastasis}},
url = {http://www.nature.com/doifinder/10.1038/nrc.2016.25},
volume = {16},
year = {2016}
}
@misc{Bankoski2011,
author = {Bankoski, J. and Koleszar, J. and Quillio, L. and Salonen, J. and Wilkins, P. and Xu, Y. and Inc., Google},
file = {:Users/octavi/Documents/Mendeley Desktop/Bankoski et al. - 2011 - VP8 Data Format and Decoding Guide.html:html},
title = {{VP8 Data Format and Decoding Guide}},
url = {http://www.rfc-editor.org/rfc/rfc6386.txt},
urldate = {2013-06-14},
year = {2011}
}
@article{Hua2015,
abstract = {Lung cancer has a poor prognosis when not diagnosed early and unresectable lesions are present. The management of small lung nodules noted on computed tomography scan is controversial due to uncertain tumor characteristics. A conventional computer-aided diagnosis (CAD) scheme requires several image processing and pattern recognition steps to accomplish a quantitative tumor differentiation result. In such an ad hoc image analysis pipeline, every step depends heavily on the performance of the previous step. Accordingly, tuning of classification performance in a conventional CAD scheme is very complicated and arduous. Deep learning techniques, on the other hand, have the intrinsic advantage of an automatic exploitation feature and tuning of performance in a seamless fashion. In this study, we attempted to simplify the image analysis pipeline of conventional CAD with deep learning techniques. Specifically, we introduced models of a deep belief network and a convolutional neural network in the context of nodule classification in computed tomography images. Two baseline methods with feature computing steps were implemented for comparison. The experimental results suggest that deep learning methods could achieve better discriminative results and hold promise in the CAD application domain.},
author = {Hua, Kai-Lung and Hsu, Che-Hao and {Chusnul Hidayati}, Shintami and Cheng, Wen-Huang and Chen, Yu-Jen},
doi = {10.2147/OTT.S80733},
file = {:Users/octavi/Documents/Mendeley Desktop/Hua et al. - 2015 - Computer-aided classification of lung nodules on computed tomography images via deep learning technique.pdf:pdf},
keywords = {convolutional neural network,deep belief network,deep learning,nodule classification},
title = {{Computer-aided classification of lung nodules on computed tomography images via deep learning technique}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4531007/pdf/ott-8-2015.pdf},
year = {2015}
}
@misc{Gregorik2012,
author = {Grigorik, Ilyia},
file = {:Users/octavi/Documents/Mendeley Desktop/Grigorik - 2012 - SPDY, err... HTTP 2.0.html:html},
title = {{SPDY, err... HTTP 2.0}},
url = {http://www.igvita.com/slides/2012/http2-spdy-devconf.pdf},
urldate = {2013-06-14},
year = {2012}
}
@book{Fisher1992,
author = {Fisher, Yuval},
booktitle = {SIGGRAPH},
file = {:Users/octavi/Documents/Mendeley Desktop/fractal{\_}image{\_}compression{\_}paper.pdf:pdf},
title = {{Fractal Image Compression}},
year = {1992}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:Users/octavi/Documents/Mendeley Desktop/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@article{Kaufmann2014,
abstract = {BACKGROUND: People with severe disabilities, e.g., due to neurodegenerative disease, depend on technology that allows for accurate wheelchair control. For those who cannot operate a wheelchair with a joystick, brain-computer interfaces (BCI) may offer a valuable option. Technology depending on visual or auditory input may not be feasible as these modalities are dedicated to processing of environmental stimuli (e.g., recognition of obstacles, ambient noise). Herein we thus validated the feasibility of a BCI based on tactually-evoked event-related potentials (ERP) for wheelchair control. Furthermore, we investigated use of a dynamic stopping method to improve speed of the tactile BCI system.$\backslash$n$\backslash$nMETHODS: Positions of four tactile stimulators represented navigation directions (left thigh: move left; right thigh: move right; abdomen: move forward; lower neck: move backward) and N = 15 participants delivered navigation commands by focusing their attention on the desired tactile stimulus in an oddball-paradigm.$\backslash$n$\backslash$nRESULTS: Participants navigated a virtual wheelchair through a building and eleven participants successfully completed the task of reaching 4 checkpoints in the building. The virtual wheelchair was equipped with simulated shared-control sensors (collision avoidance), yet these sensors were rarely needed.$\backslash$n$\backslash$nCONCLUSION: We conclude that most participants achieved tactile ERP-BCI control sufficient to reliably operate a wheelchair and dynamic stopping was of high value for tactile ERP classification. Finally, this paper discusses feasibility of tactile ERPs for BCI based wheelchair control.},
author = {Kaufmann, Tobias and Herweg, Andreas and K{\"{u}}bler, Andrea},
doi = {10.1186/1743-0003-11-7},
file = {:Users/octavi/Documents/Mendeley Desktop/1743-0003-11-7.pdf:pdf},
isbn = {1743-0003 (Electronic) 1743-0003 (Linking)},
issn = {17430003},
journal = {Journal of NeuroEngineering and Rehabilitation},
keywords = {Brain-computer interface,Dynamic stopping,Event-related potentials,P300,Tactile,Wheelchair},
number = {1},
pages = {1--17},
pmid = {24428900},
title = {{Toward brain-computer interface based wheelchair control utilizing tactually-evoked event-related potentials}},
volume = {11},
year = {2014}
}
@misc{Fieldinga,
author = {Fielding, R. and Irvine, UC and Gettys, J. and Compaq/W3C and Mogul, J. and Compaq and Frystyk, H. and W3C/MIT and Masinter, L. and Xerox and Leach, P. and Microsoft and Berners-Lee, T.},
file = {:Users/octavi/Documents/Mendeley Desktop/Fielding et al. - 1999 - RFC2616 Hypertext Transfer Protocol -- HTTP1.1.html:html},
title = {{RFC2616 Hypertext Transfer Protocol -- HTTP/1.1}},
url = {http://www.ietf.org/rfc/rfc2616.txt},
urldate = {2013-06-14},
year = {1999}
}
@article{Chen2018,
abstract = {Segmentation of key brain tissues from 3D medical images is of great significance for brain disease diagnosis, progression assessment and monitoring of neurologic conditions. While manual segmentation is time-consuming, laborious, and subjective, automated segmentation is quite challenging due to the complicated anatomical environment of brain and the large variations of brain tissues. We propose a novel voxelwise residual network (VoxResNet) with a set of effective training schemes to cope with this challenging problem. The main merit of residual learning is that it can alleviate the degradation problem when training a deep network so that the performance gains achieved by increasing the network depth can be fully leveraged. With this technique, our VoxResNet is built with 25 layers, and hence can generate more representative features to deal with the large variations of brain tissues than its rivals using hand-crafted features or shallower networks. In order to effectively train such a deep network with limited training data for brain segmentation, we seamlessly integrate multi-modality and multi-level contextual information into our network, so that the complementary information of different modalities can be harnessed and features of different scales can be exploited. Furthermore, an auto-context version of the VoxResNet is proposed by combining the low-level image appearance features, implicit shape information, and high-level context together for further improving the segmentation performance. Extensive experiments on the well-known benchmark (i.e., MRBrainS) of brain segmentation from 3D magnetic resonance (MR) images corroborated the efficacy of the proposed VoxResNet. Our method achieved the first place in the challenge out of 37 competitors including several state-of-the-art brain segmentation methods. Our method is inherently general and can be readily applied as a powerful tool to many brain-related studies, where accurate segmentation of brain structures is critical.},
archivePrefix = {arXiv},
arxivId = {1608.05895},
author = {Chen, Hao and Dou, Qi and Yu, Lequan and Qin, Jing and Heng, Pheng Ann},
doi = {10.1016/j.neuroimage.2017.04.041},
eprint = {1608.05895},
file = {:Users/octavi/Documents/Mendeley Desktop/1608.05895.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {3D deep learning,Auto-context,Brain segmentation,Convolutional neural network,Multi-level contextual information,Multi-modality,Residual learning},
pages = {446--455},
pmid = {28445774},
title = {{VoxResNet: Deep voxelwise residual networks for brain segmentation from 3D MR images}},
volume = {170},
year = {2018}
}
@article{Hegde2017,
abstract = {Why do individuals become entrepreneurs? We argue that information asymmetries and the quest to maximize pecuniary returns produces entrepreneurs. In our model, in-dividuals signal their hidden ability to employers (e.g., via educational qualifications). However, signals are imperfect and individuals with greater ability than their signals convey to employers become entrepreneurs. Empirical analysis of two longitudinal sam-ples of U.S. and U.K. residents supports the model's predictions that (i) entrepreneurs have higher ability than employees with comparable signals, (ii) employees have better signals than equally able entrepreneurs, and (iii) entrepreneurs' earnings are higher and exhibit greater variance than employees' with similar signals.},
author = {Hegde, Deepak and Tumlinson, Justin and Cattani, Gino and Eggers, J P and Goldfarb, Brent and Kerr, William and Kominers, Scott and Lerner, Josh and Ljungqvist, Alexander and Li, Danielle and Morgan, John and Nanda, Ramana and Porac, Joe and R{\o}nde, Thomas and Thompson, Peter and Triebs, Thomas and Villa-Longa, Belen and Wiegand, Manuel and Zarutskie, Rebecca},
file = {:Users/octavi/Documents/Mendeley Desktop/SSRN-id2596846.pdf:pdf},
journal = {SSRN Electronic Journal},
title = {{Asymmetric Information and Entrepreneurship}},
url = {https://poseidon01.ssrn.com/delivery.php?ID=0180870830661141000950810971181001020520270210010580010270750850640940810861220660770991180530370111110231201040660250790641111220270380890150911221241260000291070060490110631240240870031260660750061001030071041},
year = {2017}
}
@book{Schwartz2012,
author = {Schwartz, Baron and Zaitsev, Peter and Tkachenko, Vadim},
edition = {3rd},
file = {:Users/octavi/Documents/Mendeley Desktop/Schwartz, Zaitsev, Tkachenko - 2012 - High Performance MySQL.pdf:pdf},
isbn = {9781449314286},
pages = {826},
publisher = {O'Reilly},
title = {{High Performance MySQL}},
year = {2012}
}
@book{MSonka2000,
author = {{M Sonka}, JM Fitzpatrick},
title = {{Handbook of medical imaging(Volume 2, Medical image processing and analysis)}},
year = {2000}
}
@article{Shrivastava2016,
abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
archivePrefix = {arXiv},
arxivId = {1612.07828},
author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
doi = {10.1109/CVPR.2017.241},
eprint = {1612.07828},
file = {:Users/octavi/Documents/Mendeley Desktop/Shrivastava{\_}Learning{\_}From{\_}Simulated{\_}CVPR{\_}2017{\_}paper.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
title = {{Learning from Simulated and Unsupervised Images through Adversarial Training}},
url = {http://arxiv.org/abs/1612.07828},
year = {2016}
}
@book{Souders2009,
author = {Souders, Steve},
file = {:Users/octavi/Documents/Mendeley Desktop/Souders - 2009 - Even Faster Web Sites.pdf:pdf},
isbn = {9780596522308},
pages = {256},
publisher = {O'Reilly},
title = {{Even Faster Web Sites}},
year = {2009}
}
@misc{KlintFinley,
author = {Finley, Klint},
file = {:Users/octavi/Documents/Mendeley Desktop/Finley - 2013 - Facebook Makes Itself a Bit More SPDY.html:html},
title = {{Facebook Makes Itself a Bit More SPDY}},
url = {http://www.wired.com/wiredenterprise/2013/03/facebook-spdy/},
urldate = {2013-06-14},
year = {2013}
}
@article{Nagle,
author = {Nagle, J.},
title = {{Congestion Control in IP/TCP Internetworks}},
url = {http://tools.ietf.org/html/rfc896}
}
@article{Birbaumer2006,
abstract = {Brain-computer interfaces (BCI) allow control of computers or external devices with regulation of brain activity alone. Invasive BCIs, almost exclusively investigated in animal models using implanted electrodes in brain tissue, and noninvasive BCIs using electrophysiological recordings in humans are described. Clinical applications were reserved with few exceptions for the noninvasive approach: communication with the completely paralyzed and locked-in syndrome with slow cortical potentials, sensorimotor rhythm and P300, and restoration of movement and cortical reorganization in high spinal cord lesions and chronic stroke. It was demonstrated that noninvasive EEG-based BCIs allow brain-derived communication in paralyzed and locked-in patients but not in completely locked-in patients. At present no firm conclusion about the clinical utility of BCI for the control of voluntary movement can be made. Invasive multielectrode BCIs in otherwise healthy animals allowed execution of reaching, grasping, and force variations based on spike patterns and extracellular field potentials. The newly developed fMRI-BCIs and NIRS-BCIs, like EEG BCIs, offer promise for the learned regulation of emotional disorders and also disorders of young children.},
author = {Birbaumer, Niels},
doi = {10.1111/j.1469-8986.2006.00456.x},
file = {:Users/octavi/Documents/Mendeley Desktop/j.1469-8986.2006.00456.x.pdf:pdf},
isbn = {0048-5772 (Print)$\backslash$n0048-5772 (Linking)},
issn = {14698986},
journal = {Psychophysiology},
keywords = {Brain-computer interface,Brain-machine interface,EEG,Invasive brain measures,Locked-in syndrome},
number = {6},
pages = {517--532},
pmid = {17076808},
title = {{Breaking the silence: Brain-computer interfaces (BCI) for communication and motor control}},
volume = {43},
year = {2006}
}
@article{Glorot,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experi-mental results showing the superiority of deeper vs less deep architectures. All these experimen-tal results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations func-tions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into satu-ration. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during train-ing, with the idea that training may be more dif-ficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
file = {:Users/octavi/Documents/Mendeley Desktop/Glorot, Bengio - Unknown - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
title = {{Understanding the difficulty of training deep feedforward neural networks}}
}
@misc{Grigorik2012b,
author = {Grigorik, Ilyia},
file = {:Users/octavi/Documents/Mendeley Desktop/Grigorik - 2012 - Building Faster Websites crash course on web performance.html:html},
title = {{Building Faster Websites crash course on web performance}},
url = {http://www.igvita.com/slides/2012/webperf-crash-course.pdf},
urldate = {2013-06-14},
year = {2012}
}
@article{Zhang2017,
abstract = {With the ever-growing volume, complexity and dynamicity of online information, recommender system has been an effective key solution to overcome such information overload. In recent years, deep learning's revolutionary advances in speech recognition, image analysis and natural language processing have gained significant attention. Meanwhile, recent studies also demonstrate its effectiveness in coping with information retrieval and recommendation tasks. Applying deep learning techniques into recommender system has been gaining momentum due to its state-of-the-art performances and high-quality recommendations. In contrast to traditional recommendation models, deep learning provides a better understanding of user's demands, item's characteristics and historical interactions between them. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems towards fostering innovations of recommender system research. A taxonomy of deep learning based recommendation models is presented and used to categorize the surveyed articles. Open problems are identified based on the analytics of the reviewed works and potential solutions discussed.},
archivePrefix = {arXiv},
arxivId = {1707.07435},
author = {Zhang, Shuai and Yao, Lina and Sun, Aixin},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {1707.07435},
file = {:Users/octavi/Documents/Mendeley Desktop/dl2-recommender-systems-survey-2005.pdf:pdf},
isbn = {9781450335492},
issn = {15232867},
number = {1},
pages = {1--35},
title = {{Deep Learning based Recommender System: A Survey and New Perspectives}},
url = {http://arxiv.org/abs/1707.07435},
volume = {1},
year = {2017}
}
@article{Dean2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
editor = {Daniel, L Purich},
file = {:Users/octavi/Documents/Mendeley Desktop/Dean, Ghemawat - 2008 - MapReduce Simplified Data Processing on Large Clusters.pdf:pdf},
institution = {Google, Inc.},
isbn = {9781595936868},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {1--13},
pmid = {11687618},
publisher = {ACM},
series = {SIGMOD '07},
title = {{MapReduce : Simplified Data Processing on Large Clusters}},
url = {http://portal.acm.org/citation.cfm?id=1327492},
volume = {51},
year = {2008}
}
@article{Esteva2017,
abstract = {Skin cancer, the most common human malignancy1, 2, 3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4, 5 show potential for general and highly variable tasks across many fine-grained object categories6, 7, 8, 9, 10, 11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets12—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care. 最も多いヒト悪性腫瘍である皮膚がんは、主に視覚的に診断され、初期臨床スクリーニングが行われた後、ダーモスコピー解析、生検、および病理組織検査が行われることがある。画像を用いた皮膚病変の自動分類は、皮膚病変の外観には細かなばらつきがあるため、難しい課題である。深層畳み込みニューラルネットワーク（CNN）には、多くの細かな対象カテゴリー全体にわたり、一般的で変動性の高い課題をこなせる可能性がある。本研究で我々は、入力としてピクセルと疾病ラベルのみを用い、画像からエンドツーエンドで直接学習させた、単一のCNNを用いた皮膚病変の分類を実証する。我々は、2032の異なる疾病からなる12万9450の臨床画像（以前のデータセットよりも2桁多い）のデータセットを用いてCNNを学習させた。我々は生検で確認した臨床画像について、このCNNの成績を21人の皮膚科認定医と比較検証した。ここでは「角化細胞がん」対「良性脂漏性角化症」、「悪性黒色腫」対「良性母斑」という、2つの重要な二項分類使用症例を用いた。前者は最も多いがんの識別であり、後者は最も致命的な皮膚がんの識別である。CNNはどちらの課題においても、試験に参加した全ての専門家と同等の成績を達成したことから、人工知能は皮膚科医に相当する能力で皮膚がんを分類できることが示された。深層ニューラルネットワークを装備した携帯デバイスにより、皮膚科医の診察が診療所の外でも受けられる可能性がある。2021年までにスマートフォン契約者は63億人に達すると見込まれ、従って、重要な診断があらゆる場所で安価に受けられるようになるかもしれない。},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
doi = {10.1038/nature21056},
file = {:Users/octavi/Documents/Mendeley Desktop/esteva2017.pdf:pdf},
isbn = {0028-0836},
issn = {14764687},
journal = {Nature},
number = {7639},
pages = {115--118},
pmid = {28117445},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
url = {http://dx.doi.org/10.1038/nature21056},
volume = {542},
year = {2017}
}
@article{Horizon2015,
author = {Horizon, Bnci and Commission, European and Programme, Framework},
file = {:Users/octavi/Documents/Mendeley Desktop/Appendix{\_}D{\_}Use{\_}Cases{\_}Focus{\_}Groups.pdf:pdf},
title = {{BNCI Horizon 2020 The Future of Brain / Neural Computer Interaction : Horizon 2020 Appendix D Use Cases and Focus Groups}},
volume = {2020},
year = {2015}
}
@article{Levey2003,
abstract = {Chronic kidney disease is a worldwide public health problem with an increasing incidence and prevalence, poor outcomes, and high cost. Outcomes of chronic kidney disease include not only kidney failure but also complications of decreased kidney function and cardiovascular disease. Current evidence suggests that some of these adverse outcomes can be prevented or delayed by early detection and treatment. Unfortunately, chronic kidney disease is underdiagnosed and undertreated, in part as a result of lack of agreement on a definition and classification of its stages of pro- gression. Recent clinical practice guidelines by the National Kidney Foundation 1) define chronic kidney disease and classify its stages, regardless of underlying cause, 2) evaluate laboratory measurements for the clinical assessment of kidney disease, 3) associate the level of kidney function with complications of chronic kidney disease, and 4) stratify the risk for loss of kidney function and development of cardiovascular disease. The guide- lines were developed by using an approach based on the proce- dure outlined by the Agency for Healthcare Research and Quality. This paper presents the definition and five-stage classifica- tion system of chronic kidney disease and summarizes the major recommendations on early detection in adults. Recommendations include identifying persons at increased risk (those with diabetes, those with hypertension, those with a family history of chronic kidney disease, those older than 60 years of age, or those with U.S. racial or ethnic minority status), detecting kidney damage by measuring the albumin–creatinine ratio in untimed (“spot”) urine specimens, and estimating the glomerular filtration rate from se- rum creatinine measurements by using prediction equations. Be- cause of the high prevalence of early stages of chronic kidney disease in the general population (approximately 11{\%} of adults), this information is particularly important for general internists and specialists.},
author = {Levey, Andrew S. and Coresh, Josef. and Balk, Ethan. and Kausz, Annamaria T. and Levin, Adeera. and Steffes, Michael W. and Hogg, Ronald J. and Perrone, Ronald D. and Lau, Joseph and Eknoya, Garabed},
doi = {200307150-00013 [pii]},
file = {:Users/octavi/Documents/Mendeley Desktop/0000605-200307150-00013.pdf:pdf},
isbn = {1539-3704 (Electronic)},
issn = {1539-3704},
journal = {Annals of Internal Medicine},
pages = {137--147},
pmid = {12859163},
title = {{Clinical Guidelines National Kidney Foundation Practice Guidelines for Chronic Kidney}},
volume = {139},
year = {2003}
}
@misc{TheMendeleySupportTeam2011a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/octavi/Documents/Mendeley Desktop/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@misc{Turk1991,
abstract = {An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space (`face space') that best encodes the variation among known face images. The face space is defined by the `eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner},
author = {Turk, M.a. Ma and Pentland, Ap A.P.},
booktitle = {Journal of Cognitive Neuroscience},
doi = {10.1109/CVPR.1991.139758},
file = {:Users/octavi/Documents/Mendeley Desktop/Paper of eigenfaces.pdf:pdf},
isbn = {0-8186-2148-6},
issn = {1063-6919},
number = {1},
pages = {72 -- 86},
pmid = {23964806},
title = {{Face Recognition Using Eigenfaces}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=139758{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=139758},
volume = {3},
year = {1991}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:Users/octavi/Documents/Mendeley Desktop/pedregosa11a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Litt2001,
abstract = {Mechanisms underlying seizure generation are traditionally thought to act over seconds to minutes before clinical seizure onset. We analyzed continuous 3- to 14-day intracranial EEG recordings from five patients with mesial temporal lobe epilepsy obtained during evaluation for epilepsy surgery. We found localized quantitative EEG changes identifying prolonged bursts of complex epileptiform discharges that became more prevalent 7 hr before seizures and highly localized subclinical seizure-like activity that became more frequent 2 hr prior to seizure onset. Accumulated energy increased in the 50 min before seizure onset, compared to baseline. These observations, from a small number of patients, suggest that epileptic seizures may begin as a cascade of electrophysiological events that evolve over hours and that quantitative measures of preseizure electrical activity could possibly be used to predict seizures far in advance of clinical onset.},
author = {Litt, Brian and Esteller, Rosana and Echauz, Javier and D'Alessandro, Maryann and Shor, Rachel and Henry, Thomas and Pennell, Page and Epstein, Charles and Bakay, Roy and Dichter, Marc and Vachtsevanos, George},
doi = {10.1016/S0896-6273(01)00262-8},
file = {:Users/octavi/Documents/Mendeley Desktop/1-s2.0-S0896627301002628-main.pdf:pdf},
isbn = {0896-6273 (Print)},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {51--64},
pmid = {11343644},
title = {{Epileptic seizures may begin hours in advance of clinical onset: A report of five patients}},
volume = {30},
year = {2001}
}
@article{VanderWalt2014,
abstract = {scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.},
archivePrefix = {arXiv},
arxivId = {1407.6245},
author = {van der Walt, St{\'{e}}fan and Sch{\"{o}}nberger, Johannes L. and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
doi = {10.7717/peerj.453},
eprint = {1407.6245},
file = {:Users/octavi/Documents/Mendeley Desktop/peerj-453.pdf:pdf},
isbn = {2167-9843},
issn = {2167-8359},
journal = {PeerJ},
keywords = {accepted 4 june 2014,education,image processing,open source,python,reproducible research,scientific programming,submitted 2 april 2014,visualization},
pages = {e453},
pmid = {25024921},
title = {{scikit-image: image processing in Python}},
url = {https://peerj.com/articles/453},
volume = {2},
year = {2014}
}
@misc{Q-Success,
author = {Q-Success},
file = {:Users/octavi/Documents/Mendeley Desktop/Q-Success - 2013 - Usage Statistics of SPDY for Websites, June 2013.html:html},
title = {{Usage Statistics of SPDY for Websites, June 2013}},
url = {http://w3techs.com/technologies/details/ce-spdy/all/all},
urldate = {2013-06-14},
year = {2013}
}
@phdthesis{MoralLleo2018,
author = {{Moral Lleo}, Albert (UPF)},
file = {:Users/octavi/Documents/Mendeley Desktop/TFG{\_}Albert{\_}MoralLleó{\_}GET.pdf:pdf},
title = {{Creacio d'un entorn tecnologic destinat al calcul de resultats de recerca mitjancant orquestracio al nuvol}},
year = {2018}
}
@article{Belshe,
author = {Belshe, Mike and Thomson, Martin and Melnikov, Alexey and Peon, Roberto},
title = {{Hypertext Transfer Protocol version 2.0}},
url = {http://tools.ietf.org/html/draft-ietf-httpbis-http2-03},
year = {2013}
}
@article{Fernandez-Rodriguez2016,
abstract = {This paper presents a review of the state of the art regarding wheelchairs driven by a brain–computer interface. Using a brain-controlled wheelchair (BCW), disabled users could handle a wheelchair through their brain activity, granting autonomy to move through an experimental environment. A classification is established, based on the characteristics of the BCW, such as the type of electroencephalographic signal used, the navigation system employed by the wheelchair, the task for the participants, or the metrics used to evaluate the performance. Furthermore, these factors are compared according to the type of signal used, in order to clarify the differences among them. Finally, the trend of current research in this field is discussed, as well as the challenges that should be solved in the future.},
author = {Fern{\'{a}}ndez-Rodr{\'{i}}guez and Velasco-{\'{A}}lvarez, F. and Ron-Angevin, R.},
doi = {10.1088/1741-2560/13/6/061001},
file = {:Users/octavi/Documents/Mendeley Desktop/Review{\_}BCW{\_}UMA{\_}BCI.pdf:pdf},
issn = {17412552},
journal = {Journal of Neural Engineering},
keywords = {brain-computer interface (BCI),real environment,review,wheelchair},
number = {6},
pmid = {27739401},
title = {{Review of real brain-controlled wheelchairs}},
volume = {13},
year = {2016}
}
@misc{CanIUse,
author = {CanIUse},
file = {:Users/octavi/Documents/Mendeley Desktop/CanIUse - Unknown - Can I use SPDY networking protocol.html:html},
keywords = {web browser compatibility support html css svg htm},
title = {{Can I use SPDY networking protocol}},
url = {http://caniuse.com/spdy},
urldate = {2013-06-14}
}
@article{Keane1999a,
abstract = {Considerable evidence, accrued over the past decade, indicates that the presence of even relatively small amounts of protein or albumin in the urine is an important early sign of kidney disease and is a strong predictor of an increased risk for file:///Users/octavi/Desktop/keane1999.pdfcardiovascular mortality and morbidity in certain high-risk groups within the general population. The early detection of proteinuria/albuminuria and institution of appropriate therapy has been shown to slow the progression of kidney disease. Similarly, the presence of abnormal quantities of protein in the urine identifies those who are at increased risk for myocardial infarction and stroke, and intensive management of such risk factors as high blood pressure and abnormal lipids may be of great benefit to these high-risk individuals. To make this information more available to the profession and the public, the National Kidney Foundation (NKF) convened a conference titled “Proteinuria, Albuminuria, Risk, Assessment, Detection and Elimination” (PARADE), in Nashville, Tennessee, on March 25 and 26, 1998. Three separate panels of experts and invited participants addressed issues related to proteinuria as a risk factor for cardiovascular disease, proteinuria as a mediator and marker of progressive kidney disease, and persistent massive proteinuria as the inciting factor that leads to the nephrotic syndrome. The initial expert recommendations that were developed for the evaluation and management of adults with proteinuria were presented at four Town Hall meetings in Minneapolis, Minnesota; San Antonio, Texas; Memphis, Tennessee; and Washington, DC, for additional input, comment, or further modification. These revised recommendations were submitted for review by the Scientific Advisory Board of the NKF and adopted as a position paper by the Board of Directors of the NKF at its meeting in Philadelphia on October 23, 1998. They will be submitted for review and comment to appropriate federal, medical, and public organizations before wider dissemination. In due course, a structured review of the subject will be necessary to develop guidelines. Specific recommendations for children and adolescents are in preparation.},
author = {Keane, William F. and Eknoyan, Garabed},
doi = {10.1016/S0272-6386(99)70442-7},
issn = {1523-6838},
journal = {American journal of kidney diseases: the official journal of the National Kidney Foundation},
keywords = {Adult,Albuminuria,Cardiovascular Diseases,Cholesterol,Creatinine,Diet,Dietary Proteins,Humans,Hypertension,Kidney Diseases,Nephrotic Syndrome,Practice Guidelines as Topic,Proteinuria,Risk Factors,Serum Albumin,Sodium-Restricted},
month = {may},
number = {5},
pages = {1004--1010},
pmid = {10213663},
title = {{Proteinuria, albuminuria, risk, assessment, detection, elimination (PARADE): a position paper of the National Kidney Foundation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0272638699704427 http://www.ncbi.nlm.nih.gov/pubmed/10213663},
volume = {33},
year = {1999}
}
@article{Seltzer1995,
abstract = {PURPOSE: To determine radiologists' ability to find lung nodules on spiral computed tomographic (CT) scans of the chest with both rapid sequential (cine) and conventional film-based viewing. MATERIALS AND METHODS: Eight radiologists searched for lung nodules on spiral CT images (10-mm collimation, 10 mm/sec table speed) presented in two formats. Cine viewing was performed at a computer work-station; sections were viewed in 2-mm increments at frame rates up to 10 frames per second. Film-based viewing of images from a laser printer was performed with a lightbox; sections were viewed at 4-mm increments. Eight 3-5-mm-diameter simulated nodules were superimposed on each of five normal CT scans. RESULTS: Radiologists found a higher fraction of nodules with the cine presentation than with film (mean, 0.69 +/- 0.02 [standard error] versus 0.58 +/- 0.03, respectively [P = .006]). Diameter thresholds for nodule detection (50{\%} correctly localized) were 3.3 and 3.5 mm, respectively. CONCLUSION: Cine viewing of spiral CT images of the chest improved radiologists' ability to detect nodules.},
author = {Seltzer, S E and Judy, P F and Adams, D F and Jacobson, F L and Stark, P and Kikinis, R and Swensson, R G and Hooton, S and Head, B and Feldman, U},
doi = {10.1148/radiology.197.1.7568857},
file = {:Users/octavi/Documents/Mendeley Desktop/seltzer1995.pdf:pdf},
isbn = {0033-8419 (Print)$\backslash$r0033-8419 (Linking)},
issn = {0033-8419},
journal = {Radiology},
keywords = {Humans,Solitary Pulmonary Nodule/ radiography,Tomography, X-Ray Computed/ methods},
number = {1},
pages = {73--78},
pmid = {7568857},
title = {{Spiral CT of the chest: comparison of cine and film-based viewing}},
volume = {197},
year = {1995}
}
@article{Muller-Putz2015,
abstract = {The main objective of this roadmap is to provide a global perspective on the BCI field now and in the future. For readers not familiar with BCIs, we introduce basic terminology and concepts. We discuss what BCIs are, what BCIs can do, and who can benefit from BCIs. We illustrate our arguments with use cases to support the main messages. After reading this roadmap you will have a clear picture of the potential benefits and challenges of BCIs, the steps necessary to bridge the gap between current and future applications, and the potential impact of BCIs on society in the next decade and beyond.},
author = {Muller-Putz, G. R. and Brunner, Clemens and Bauernfeind, G. and Blefari, M. L. and Mill{\'{a}}n, Jos{\'{e}} del R. and Real, R. G. L. and K{\"{u}}bler, Andrea and Mattia, Donatella and Pichiorri, F. and Schettini, F. and Ramsey, Nick and H{\"{o}}hne, J. and Blankertz, Benjamin and Miralles, Felip and Otal, B. and Guger, Christoph and Ortner, Rupert and Poel, Mannes and Nijholt, Anton and Reuderink, Boris and Birbaumer, Niels and de Pobes, A. and Salomon, Patric and van Steensel, M. and Soekader, S. and Opisso, E.},
doi = {10.3390/s140814601.Allison},
file = {:Users/octavi/Documents/Mendeley Desktop/Appendix{\_}A{\_}Research.pdf:pdf},
isbn = {9783851253795},
keywords = {Brain-Computer Interfaces},
pages = {1--48},
title = {{BNCI Horizon 2020 The Future of Brain / Neural Computer Interaction : Horizon 2020 Appendix A Research}},
url = {http://dx.doi.org/10.3217/978-3-85125-379-5{\%}5Cnhttp://bnci-horizon-2020.eu/images/bncih2020/Appendix{\_}A{\_}Research.pdf},
volume = {2020},
year = {2015}
}
@book{Kurose2010,
author = {Kurose, James F. and Ross, Keith W.},
edition = {5th},
pages = {888},
publisher = {Person},
title = {{Computer Networking A top-down approach}},
year = {2010}
}
@book{Jacobs2015,
abstract = {Automatic analysis of thoracic CT scans has the potential to improve detection rate of pulmonary nodules, reduce interobserver variability and speed up evaluation of screening CT scans. This however strongly depends on the performance of CAD systems. If the performance is good enough, it will probably positively influence the cost-effectiveness of lung cancer CT screening. This thesis presents novel algorithms to find abnormalities in thoracic CT images, which may be of benefit in the interpretation of chest CTs in lung cancer screening or clinical practice in general. The main focus of the thesis is automatic detection of pulmonary nodules in thoracic CT. The outline of this thesis is as follows. Chapter 2 describes a novel computer-aided detection system for subsolid pulmonary nodules. Detection of subsolid nodules has increased due to the use of thin-slice CT and the implementation of lung cancer screening trials and as a consequence, their prevalence and malignancy rate are better understood. Subsolid nodules are less common, but show a higher malignancy rate than solid nodules. We present a novel subsolid CAD system which is trained and validated with data from two sites from the NELSON lung cancer screening trial. In Chapter 3, a nodule detection system is developed for the detection of micronodules in subjects at high-risk for developing silicosis. Chronic silicosis is radiologically characterized by widespread, well-defined solid pulmonary micronodules, measuring 3 mm or less. Early detection is crucial to stop progression but detection and quantification of these small nodules is tedious for human observers. We present an automatic method which finds micronodules and quantifies the micronodule load. Chapter 4 describes a study in which we perform a comparative study of state-of-the-art nodule CAD systems on the largest publicly available reference data set, containing more than 1,000 CT scans. We perform an extensive analysis of the performance of three state-of-the-art CAD systems; two commercial and one academic CAD system. In Chapter 5, an automatic classification system for pulmonary nodules detected on CT is presented. Every nodule is classified as either solid, part-solid, or non-solid. This classification is crucial for management of pulmonary nodules. To put the performance of CAD into context, we also assess the interobserver variability among radiologists in this chapter. In Chapter 6, a system for automatic detection of interval change on consecutive CT scans is proposed. Interval change is of major importance when subjects are screened repeatedly. We propose an automatic detection system based on subtraction images which are acquired by employing an elastic registration method between consecutive CT images. Chapter 7 discusses how the presented algorithms could be efficiently integrated into software that could be used in clinical routine or in CT lung screening programs.},
author = {Jacobs, Colin},
file = {:Users/octavi/Documents/Mendeley Desktop/Colin-thesis2015.pdf:pdf},
isbn = {9789462598805},
keywords = {ISICAD},
mendeley-tags = {ISICAD},
title = {{Automatic detection and characterization of pulmonary nodules in thoracic CT scans}},
year = {2015}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:Users/octavi/Documents/Mendeley Desktop/reluICML.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/octavi/Documents/Mendeley Desktop/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@misc{Paul,
author = {Paul, Ryan},
file = {:Users/octavi/Documents/Mendeley Desktop/Paul - 2011 - Amazon's Silk Web browser adds new twist to old idea Ars Technica.html:html},
title = {{Amazon's Silk Web browser adds new twist to old idea | Ars Technica}},
url = {http://arstechnica.com/gadgets/2011/09/amazons-silk-web-browser-adds-new-twist-to-old-idea/},
urldate = {2013-06-14},
year = {2011}
}
@misc{Google,
author = {{Google Inc.}},
file = {:Users/octavi/Documents/Mendeley Desktop/Google Inc. - 2012 - WebP — A new image format for the Web.html:html},
title = {{WebP — A new image format for the Web}},
url = {https://developers.google.com/speed/webp/},
urldate = {2013-06-14},
year = {2012}
}
@article{AlMohammad2017,
abstract = {Lung cancer is the leading cause of cancer-related death worldwide; however, early diagnosis of lung cancer leads to higher survival rates. The National Lung Screening Trial (NLST) demonstrated that scanning with low-dose computed tomography (LDCT) led to a 20{\%} reduction in mortality rate in a high-risk population. This paper covers new developments in screening eligibility criteria and the possible benefits and the harm of screening with CT. To make the screening process more feasible and help reduce the rate of missed lung nodules, computer-aided detection (CAD) has been introduced to assist radiologists in lung nodule detection. The aim of this paper is to review how CAD works, its performance in lung nodule detection, and the factors that influence its performance. This paper also aims to investigate the effect of different types of CAD on CT in lung nodule detection and the effect of CAD on radiologists' decision outcomes.},
author = {{Al Mohammad}, B. and Brennan, P. C. and Mello-Thoms, C.},
doi = {10.1016/j.crad.2017.01.002},
file = {:Users/octavi/Documents/Mendeley Desktop/1-s2.0-S0009926017300296-main.pdf:pdf},
isbn = {1365-229X (Electronic)0009-9260 (Linking)},
issn = {1365229X},
journal = {Clinical Radiology},
number = {6},
pages = {433--442},
pmid = {28185635},
publisher = {The Royal College of Radiologists},
title = {{A review of lung cancer screening and the role of computer-aided detection}},
url = {http://dx.doi.org/10.1016/j.crad.2017.01.002},
volume = {72},
year = {2017}
}
@article{Jemal2010,
abstract = {While incidence and mortality rates for most cancers (including lung, colorectum, female breast, and prostate) are decreasing in the United States and many other western countries, they are increasing in several less developed and economically transitioning countries because of adoption of unhealthy western lifestyles such as smoking and physical inactivity and consumption of calorie-dense food. Indeed, the rates for lung and colon cancers in a few of these countries have already surpassed those in the United States and other western countries. Most developing countries also continue to be disproportionately affected by cancers related to infectious agents, such as cervix, liver, and stomach cancers. The proportion of new cancer cases diagnosed in less developed countries is projected to increase from about 56{\%} of the world total in 2008 to more than 60{\%} in 2030 because of the increasing trends in cancer rates and expected increases in life expectancy and growth of the population. In this review, we describe these changing global incidence and mortality patterns for select common cancers and the opportunities for cancer prevention in developing countries.},
author = {Jemal, Ahmedin and Center, Melissa M. and DeSantis, Carol and Ward, Elizabeth M.},
doi = {10.1158/1055-9965.EPI-10-0437},
file = {:Users/octavi/Documents/Mendeley Desktop/1893.full.pdf:pdf},
isbn = {1538-7755 (Electronic)$\backslash$n1055-9965 (Linking)},
issn = {10559965},
journal = {Cancer Epidemiology Biomarkers and Prevention},
number = {8},
pages = {1893--1907},
pmid = {20647400},
title = {{Global patterns of cancer incidence and mortality rates and trends}},
volume = {19},
year = {2010}
}
@book{Allspaw2010,
author = {Allspaw, John and Robbins, Jesse},
file = {:Users/octavi/Documents/Mendeley Desktop/Allspaw, Robbins - 2010 - Web Operations Keeping the Data on Time.pdf:pdf},
isbn = {9781449377441},
title = {{Web Operations: Keeping the Data on Time}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=0oXB7aFQ1FwC{\&}oi=fnd{\&}pg=PR11{\&}dq=Web+Operations+Keeping+the+Data+on+Time{\&}ots=3h4t7XNFGN{\&}sig=5spGZq-bDcb2pjGoBP9StBJv4r8},
year = {2010}
}
@article{Sector2013,
author = {Sector, Standardization and Itu, O F},
file = {:Users/octavi/Documents/Mendeley Desktop/Sector, Itu - 2013 - H.265 High efficiency video coding standard.pdf:pdf},
keywords = {265,H,H.265},
title = {{H.265 High efficiency video coding standard}},
volume = {265},
year = {2013}
}
@article{Iturrate2009,
abstract = {This paper describes a new noninvasive brain-actuated wheelchair that relies on a P300 neurophysiological protocol and automated navigation. When in operation, the user faces a screen displaying a real-time virtual reconstruction of the scenario and concentrates on the location of the space to reach. A visual stimulation process elicits the neurological phenomenon, and the electroencephalogram (EEG) signal processing detects the target location. This location is transferred to the autonomous navigation system that drives the wheelchair to the desired location while avoiding collisions with obstacles in the environment detected by the laser scanner. This concept gives the user the flexibility to use the device in unknown and evolving scenarios. The prototype was validated with five healthy participants in three consecutive steps: screening (an analysis of three different groups of visual interface designs), virtual-environment driving, and driving sessions with the wheelchair. On the basis of the results, this paper reports the following evaluation studies: 1) a technical evaluation of the device and all functionalities; 2) a users' behavior study; and 3) a variability study. The overall result was that all the participants were able to successfully operate the device with relative ease, thus showing a great adaptation as well as a high robustness and low variability of the system.},
author = {Iturrate, I{\~{n}}aki and Antelis, Jevier M. and K{\"{u}}bler, Andrea and Minguez, Javier},
doi = {10.1109/TRO.2009.2020347},
file = {:Users/octavi/Documents/Mendeley Desktop/04914843.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Neurorobotics,Rehabilitation robotics},
number = {3},
pages = {614--627},
title = {{A noninvasive brain-actuated wheelchair based on a P300 neurophysiological protocol and automated navigation}},
volume = {25},
year = {2009}
}
@article{Chua,
author = {Chu, Jerry and Cheng, Yuchung and Dukkipati, Nandita and Mathis, Matt},
title = {{Increasing TCP's Initial Window}},
url = {https://tools.ietf.org/html/rfc6928},
year = {2013}
}
@article{Murphy2009,
abstract = {A scheme for the automatic detection of nodules in thoracic computed tomography scans is presented and extensively evaluated. The algorithm uses the local image features of shape index and curvedness in order to detect candidate structures in the lung volume and applies two successive k-nearest-neighbour classifiers in the reduction of false-positives. The nodule detection system is trained and tested on three databases extracted from a large-scale experimental screening study. The databases are constructed in order to evaluate the algorithm on both randomly chosen screening data as well as data containing higher proportions of nodules requiring follow-up. The system results are extensively evaluated including performance measurements on specific nodule types and sizes within the databases and on lesions which later proved to be malignant. In a random selection of 813 scans from the screening study a sensitivity of 80{\%} with an average 4.2 false-positives per scan is achieved. The detection results presented are a realistic measure of a CAD system performance in a low-dose screening study which includes a diverse array of nodules of many varying sizes, types and textures. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Murphy, K. and van Ginneken, B. and Schilham, A.M.R. and de Hoop, B.J. and Gietema, H.A. and Prokop, M.},
doi = {10.1016/j.media.2009.07.001},
file = {:Users/octavi/Documents/Mendeley Desktop/murphy2009.pdf:pdf},
isbn = {1361-8415},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {ISICAD},
mendeley-tags = {ISICAD},
number = {5},
pages = {757--770},
pmid = {19646913},
publisher = {Elsevier B.V.},
title = {{A large-scale evaluation of automatic pulmonary nodule detection in chest CT using local image features and k-nearest-neighbour classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1361841509000516},
volume = {13},
year = {2009}
}
@article{Messay2015,
abstract = {We present new pulmonary nodule segmentation algorithms for computed tomography (CT). These include a fully-automated (FA) system, a semi-automated (SA) system, and a hybrid system. Like most traditional systems, the new FA system requires only a single user-supplied cue point. On the other hand, the SA system represents a new algorithm class requiring 8 user-supplied control points. This does increase the burden on the user, but we show that the resulting system is highly robust and can handle a variety of challenging cases. The proposed hybrid system starts with the FA system. If improved segmentation results are needed, the SA system is then deployed. The FA segmentation engine has 2 free parameters, and the SA system has 3. These parameters are adaptively determined for each nodule in a search process guided by a regression neural network (RNN). The RNN uses a number of features computed for each candidate segmentation. We train and test our systems using the new Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) data. To the best of our knowledge, this is one of the first nodule-specific performance benchmarks using the new LIDC-IDRI dataset. We also compare the performance of the proposed methods with several previously reported results on the same data used by those other methods. Our results suggest that the proposed FA system improves upon the state-of-the-art, and the SA system offers a considerable boost over the FA system.},
annote = {nodule segmentation mask},
author = {Messay, Temesguen and Hardie, Russell C. and Tuinstra, Timothy R.},
doi = {10.1016/j.media.2015.02.002},
file = {:Users/octavi/Documents/Mendeley Desktop/messay2015.pdf:pdf},
isbn = {1361-8415},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computed tomography,Database resource initiative,LIDC-IDRI,Lung image database consortium and image,Pulmonary nodule,Segmentation},
number = {1},
pages = {48--62},
pmid = {25791434},
publisher = {Elsevier B.V.},
title = {{Segmentation of pulmonary nodules in computed tomography using a regression neural network approach and its application to the Lung Image Database Consortium and Image Database Resource Initiative dataset}},
url = {http://dx.doi.org/10.1016/j.media.2015.02.002},
volume = {22},
year = {2015}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:Users/octavi/Documents/Mendeley Desktop/ADAM.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@misc{NVIDIA2018,
author = {NVIDIA},
title = {nvidia docker},
year = {2018}
}
@article{Dou,
abstract = {In this paper, we propose a novel framework with 3D con-volutional networks (ConvNets) for automated detection of pulmonary nodules from low-dose CT scans, which is a challenging yet crucial task for lung cancer early diagnosis and treatment. Different from previous standard ConvNets, we try to tackle the severe hard/easy sample imbal-ance problem in medical datasets and explore the benefits of localized an-notations to regularize the learning, and hence boost the performance of ConvNets to achieve more accurate detections. Our proposed framework consists of two stages: 1) candidate screening, and 2) false positive re-duction. In the first stage, we establish a 3D fully convolutional network, effectively trained with an online sample filtering scheme, to sensitively and rapidly screen the nodule candidates. In the second stage, we design a hybrid-loss residual network which harnesses the location and size infor-mation as important cues to guide the nodule recognition procedure. Ex-perimental results on the public large-scale LUNA16 dataset demonstrate superior performance of our proposed method compared with state-of-the-art approaches for the pulmonary nodule detection task.},
author = {Dou, Qi and Chen, Hao and Jin, Yueming and Lin, Huangjing and Qin, Jing and Heng, Pheng-Ann},
file = {:Users/octavi/Documents/Mendeley Desktop/Dou et al. - Unknown - Automated Pulmonary Nodule Detection via 3D ConvNets with Online Sample Filtering and Hybrid-Loss Residual Learni.pdf:pdf},
title = {{Automated Pulmonary Nodule Detection via 3D ConvNets with Online Sample Filtering and Hybrid-Loss Residual Learning}},
url = {https://arxiv.org/pdf/1708.03867.pdf}
}
@article{Cohen2018,
abstract = {Earlier detection is key to reducing cancer deaths. Here we describe a blood test that can detect eight common cancer types through assessment of the levels of circulating proteins and mutations in cell-free DNA. We applied this test, called CancerSEEK, to 1,005 patients with non-metastatic, clinically detected cancers of the ovary, liver, stomach, pancreas, esophagus, colorectum, lung, or breast. CancerSEEK tests were positive in a median of 70{\%} of the eight cancer types. The sensitivities ranged from 69{\%} to 98{\%} for the detection of five cancer types (ovary, liver, stomach, pancreas, and esophagus) for which there are no screening tests available for average-risk individuals. The specificity of CancerSEEK was {\textgreater} 99{\%}: only 7 of 812 healthy controls scored positive. In addition, CancerSEEK localized the cancer to a small number of anatomic sites in a median of 83{\%} of the patients.},
author = {Cohen, Joshua D. and Li, Lu and Wang, Yuxuan and Thoburn, Christopher and Afsari, Bahman and Danilova, Ludmila and Douville, Christopher and Javed, Ammar A. and Wong, Fay and Mattox, Austin and Hruban, Ralph. H. and Wolfgang, Christopher L. and Goggins, Michael G. and {Dal Molin}, Marco and Wang, Tian-Li and Roden, Richard and Klein, Alison P. and Ptak, Janine and Dobbyn, Lisa and Schaefer, Joy and Silliman, Natalie and Popoli, Maria and Vogelstein, Joshua T. and Browne, James D. and Schoen, Robert E. and Brand, Randall E. and Tie, Jeanne and Gibbs, Peter and Wong, Hui-Li and Mansfield, Aaron S. and Jen, Jin and Hanash, Samir M. and Falconi, Massimo and Allen, Peter J. and Zhou, Shibin and Bettegowda, Chetan and Diaz, Luis and Tomasetti, Cristian and Kinzler, Kenneth W. and Vogelstein, Bert and Lennon, Anne Marie and Papadopoulos, Nickolas},
doi = {10.1126/science.aar3247},
file = {:Users/octavi/Documents/Mendeley Desktop/10.1126@science.aar3247.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {January},
pages = {eaar3247},
title = {{Detection and localization of surgically resectable cancers with a multi-analyte blood test}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aar3247},
volume = {3247},
year = {2018}
}
@article{Valente2016,
abstract = {This work presents a systematic review of techniques for the 3D automatic detection of pulmonary nodules in computerized-tomography (CT) images. Its main goals are to analyze the latest technology being used for the development of computational diagnostic tools to assist in the acquisition, storage and, mainly, processing and analysis of the biomedical data. Also, this work identifies the progress made, so far, evaluates the challenges to be overcome and provides an analysis of future prospects. As far as the authors know, this is the first time that a review is devoted exclusively to automated 3D techniques for the detection of pulmonary nodules from lung CT images, which makes this work of noteworthy value. The research covered the published works in the Web of Science, PubMed, Science Direct and IEEEXplore up to December 2014. Each work found that referred to automated 3D segmentation of the lungs was individually analyzed to identify its objective, methodology and results. Based on the analysis of the selected works, several studies were seen to be useful for the construction of medical diagnostic aid tools. However, there are certain aspects that still require attention such as increasing algorithm sensitivity, reducing the number of false positives, improving and optimizing the algorithm detection of different kinds of nodules with different sizes and shapes and, finally, the ability to integrate with the Electronic Medical Record Systems and Picture Archiving and Communication Systems. Based on this analysis, we can say that further research is needed to develop current techniques and that new algorithms are needed to overcome the identified drawbacks.},
author = {Valente, Igor Rafael S. and Cortez, Paulo C{\'{e}}sar and Neto, Edson Cavalcanti and Soares, Jos{\'{e}} Marques and de Albuquerque, Victor Hugo C. and Tavares, Jo{\~{a}}o Manuel R.S.},
doi = {10.1016/j.cmpb.2015.10.006},
file = {:Users/octavi/Documents/Mendeley Desktop/1-s2.0-S0169260715300298-main.pdf:pdf},
isbn = {0169-2607},
issn = {18727565},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {3D image segmentation,Computer-aided detection systems,Lung cancer,Medical image analysis,Pulmonary nodules},
pages = {91--107},
pmid = {26652979},
publisher = {Elsevier Ireland Ltd},
title = {{Automatic 3D pulmonary nodule detection in CT images: A survey}},
url = {http://dx.doi.org/10.1016/j.cmpb.2015.10.006},
volume = {124},
year = {2016}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois and Others},
howpublished = {$\backslash$url{\{}https://keras.io{\}}},
title = {{Keras}},
year = {2015}
}
@misc{Belsheb,
author = {Belshe, Mike and Peon, Roberto},
file = {:Users/octavi/Documents/Mendeley Desktop/Belshe, Peon - 2009 - SPDY An experimental protocol for a faster web - The Chromium Projects.html:html},
title = {{SPDY: An experimental protocol for a faster web - The Chromium Projects}},
url = {http://www.chromium.org/spdy/spdy-whitepaper},
urldate = {2013-06-14},
year = {2009}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Y and Simard, P and Frasconi, P},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:Users/octavi/Documents/Mendeley Desktop/00279181.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Moxon2015,
abstract = {The field of invasive brain-machine interfaces (BMIs) is typically associated with neuroprosthetic applications aiming to recover loss of motor function. However, BMIs also represent a powerful tool to address fundamental questions in neuroscience. The observed subjects of BMI experiments can also be considered as indirect observers of their own neurophysiological activity, and the relationship between observed neurons and (artificial) behavior can be genuinely causal rather than indirectly correlative. These two characteristics defy the classical object-observer duality, making BMIs particularly appealing for investigating how information is encoded and decoded by neural circuits in real time, how this coding changes with physiological learning and plasticity, and how it is altered in pathological conditions. Within neuroengineering, BMI is like a tree that opens its branches into many traditional engineering fields, but also extends deep roots into basic neuroscience beyond neuroprosthetics.},
author = {Moxon, Karen A. and Foffani, Guglielmo},
doi = {10.1016/j.neuron.2015.03.036},
file = {:Users/octavi/Documents/Mendeley Desktop/1-s2.0-S0896627315002603-main.pdf:pdf},
isbn = {0896-6273},
issn = {10974199},
journal = {Neuron},
number = {1},
pages = {55--67},
pmid = {25856486},
publisher = {Elsevier Inc.},
title = {{Brain-machine interfaces beyond neuroprosthetics}},
url = {http://dx.doi.org/10.1016/j.neuron.2015.03.036},
volume = {86},
year = {2015}
}
@article{Liao2017,
abstract = {Automatic diagnosing lung cancer from Computed Tomography (CT) scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the whole-lung/pulmonary malignancy. Currently, there are many studies about the first step, but few about the second step. Since the existence of nodule does not definitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3D deep neural network to solve this problem. The model consists of two modules. The first one is a 3D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top five nodules based on the detection confidence, evaluates their cancer probabilities and combines them with a leaky noisy-or gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modified U-net. The over-fitting caused by the shortage of training data is alleviated by training the two modules alternately. The proposed model won the first place in the Data Science Bowl 2017 competition. The code has been made publicly available.},
annote = {1st on Kaggle DSB17 leaderboard},
archivePrefix = {arXiv},
arxivId = {1711.08324},
author = {Liao, Fangzhou and Liang, Ming and Li, Zhe and Hu, Xiaolin and Song, Sen},
eprint = {1711.08324},
file = {:Users/octavi/Documents/Mendeley Desktop/1711.08324.pdf:pdf},
number = {8},
pages = {1--12},
title = {{Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network}},
url = {http://arxiv.org/abs/1711.08324},
volume = {14},
year = {2017}
}
@misc{Wo,
author = {{World Wide Web Consortium (W3C)}},
file = {:Users/octavi/Documents/Mendeley Desktop/World Wide Web Consortium (W3C) - 2011 - HTTP 0.9 Summary.html:html},
title = {{HTTP 0.9 Summary}},
url = {http://www.w3.org/DesignIssues/HTTP0.9Summary.html},
urldate = {2013-06-14},
year = {2011}
}
@article{Cheng2016,
abstract = {This paper performs a comprehensive study on the deep-learning-based computer-aided diagnosis (CADx) for the differential diagnosis of benign and malignant nodules/lesions by avoiding the potential errors caused by inaccurate image processing results (e.g., boundary segmentation), as well as the classification bias resulting from a less robust feature set, as involved in most conventional CADx algorithms. Specifically, the stacked denoising auto-encoder (SDAE) is exploited on the two CADx applications for the differentiation of breast ultrasound lesions and lung CT nodules. The SDAE architecture is well equipped with the automatic feature exploration mechanism and noise tolerance advantage, and hence may be suitable to deal with the intrinsically noisy property of medical image data from various imaging modalities. To show the outperformance of SDAE-based CADx over the conventional scheme, two latest conventional CADx algorithms are implemented for comparison. 10 times of 10-fold cross-validations are conducted to illustrate the efficacy of the SDAE-based CADx algorithm. The experimental results show the significant performance boost by the SDAE-based CADx algorithm over the two conventional methods, suggesting that deep learning techniques can potentially change the design paradigm of the CADx systems without the need of explicit design and selection of problem-oriented features.},
author = {Cheng, Jie-Zhi and Ni, Dong and Chou, Yi-Hong and Qin, Jing and Tiu, Chui-Mei and Chang, Yeun-Chung and Huang, Chiun-Sheng and Shen, Dinggang and Chen, Chung-Ming},
doi = {10.1038/srep24454},
file = {:Users/octavi/Documents/Mendeley Desktop/out (1).pdf:pdf},
isbn = {2045-2322},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {24454},
pmid = {27079888},
publisher = {Nature Publishing Group},
title = {{Computer-Aided Diagnosis with Deep Learning Architecture: Applications to Breast Lesions in US Images and Pulmonary Nodules in CT Scans}},
url = {http://www.nature.com/articles/srep24454},
volume = {6},
year = {2016}
}
@article{VanGinneken2010,
abstract = {Numerous publications and commercial systems are available that deal with automatic detection of pulmonary nodules in thoracic computed tomography scans, but a comparative study where many systems are applied to the same data set has not yet been performed. This paper introduces ANODE09 ( http://anode09.isi.uu.nl), a database of 55 scans from a lung cancer screening program and a web-based framework for objective evaluation of nodule detection algorithms. Any team can upload results to facilitate benchmarking. The performance of six algorithms for which results are available are compared; five from academic groups and one commercially available system. A method to combine the output of multiple systems is proposed. Results show a substantial performance difference between algorithms, and demonstrate that combining the output of algorithms leads to marked performance improvements. {\textcopyright} 2010 Elsevier B.V.},
author = {van Ginneken, Bram and Armato, Samuel G. and de Hoop, Bartjan and {van Amelsvoort-van de Vorst}, Saskia and Duindam, Thomas and Niemeijer, Meindert and Murphy, Keelin and Schilham, Arnold and Retico, Alessandra and Fantacci, Maria Evelina and Camarlinghi, Niccol and Bagagli, Francesco and Gori, Ilaria and Hara, Takeshi and Fujita, Hiroshi and Gargano, Gianfranco and Bellotti, Roberto and Tangaro, Sabina and Bolaos, Lourdes and Carlo, Francesco De and Cerello, Piergiorgio and {Cristian Cheran}, Sorin and {Lopez Torres}, Ernesto and Prokop, Mathias},
doi = {10.1016/j.media.2010.05.005},
file = {:Users/octavi/Documents/Mendeley Desktop/1-s2.0-S1361841510000587-main.pdf:pdf},
isbn = {1361-8415},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Computed tomography,Computer-aided detection,Lung cancer,Lung nodules},
number = {6},
pages = {707--722},
pmid = {20573538},
publisher = {Elsevier B.V.},
title = {{Comparing and combining algorithms for computer-aided detection of pulmonary nodules in computed tomography scans: The ANODE09 study}},
url = {http://dx.doi.org/10.1016/j.media.2010.05.005},
volume = {14},
year = {2010}
}
@misc{Nielsen,
annote = {Summary: Slow page rendering today is typically caused by server delays or overly fancy page widgets, not by big images. Users still hate slow sites and don{\&}{\#}039;t hesitate telling us.},
author = {Nielsen, Jakob},
file = {:Users/octavi/Documents/Mendeley Desktop/Nielsen - 2010 - Website Response Times.html:html},
keywords = {JavaScript,STM,attention,brand as experience,brand attributes,branding,delays,download delays,download time,eyetracking,images,memory,performance,response time,response time limits,response times,responsiveness,server delays,short-term memory,sluggishness,speed,user control,widgets},
title = {{Website Response Times}},
url = {http://www.nngroup.com/articles/website-response-times/},
urldate = {2013-05-18},
year = {2010}
}
@article{Chaudhary2017,
abstract = {Despite partial success, communication has remained impossible for persons suffering from complete motor paralysis but intact cognitive and emotional processing, a state called complete locked-in state (CLIS). Based on a motor learning theoretical context and on the failure of neuroelectric brain-computer interface (BCI) communication attempts in CLIS, we here report BCI communication using functional near-infrared spectroscopy (fNIRS) and an implicit attentional processing procedure. Four patients suffering from advanced amyotrophic lateral sclerosis (ALS)-two of them in permanent CLIS and two entering the CLIS without reliable means of communication-learned to answer personal questions with known answers and open questions all requiring a "yes" or "no" thought using frontocentral oxygenation changes measured with fNIRS. Three patients completed more than 46 sessions spread over several weeks, and one patient (patient W) completed 20 sessions. Online fNIRS classification of personal questions with known answers and open questions using linear support vector machine (SVM) resulted in an above-chance-level correct response rate over 70{\%}. Electroencephalographic oscillations and electrooculographic signals did not exceed the chance-level threshold for correct communication despite occasional differences between the physiological signals representing a "yes" or "no" response. However, electroencephalogram (EEG) changes in the theta-frequency band correlated with inferior communication performance, probably because of decreased vigilance and attention. If replicated with ALS patients in CLIS, these positive results could indicate the first step towards abolition of complete locked-in states, at least for ALS.},
author = {Chaudhary, Ujwal and Xia, Bin and Silvoni, Stefano and Cohen, Leonardo G. and Birbaumer, Niels},
doi = {10.1371/journal.pbio.1002593},
file = {:Users/octavi/Documents/Mendeley Desktop/journal.pbio.1002593.pdf:pdf},
isbn = {1111111111},
issn = {15457885},
journal = {PLoS Biology},
number = {1},
pages = {1--25},
pmid = {28141803},
title = {{Brain–Computer Interface–Based Communication in the Completely Locked-In State}},
volume = {15},
year = {2017}
}
@misc{Gentilcore,
author = {Gentilcore, Tony},
file = {:Users/octavi/Documents/Mendeley Desktop/Gentilcore - 2010 - Running scripts in WebKit.html:html},
title = {{Running scripts in WebKit}},
url = {https://www.webkit.org/blog/1395/running-scripts-in-webkit/},
urldate = {2013-06-15},
year = {2010}
}
@article{VanHecke2008,
abstract = {Voxel based morphometry (VBM) has been increasingly applied to detect diffusion tensor (DT) image abnormalities in patients for different pathologies. An important requisite for a robust VBM analysis is the availability of a high-dimensional non-rigid coregistration technique that is able to align both the spatial and the orientational DT information. Consequently, there is a need for an inter-subject DTI atlas as a group specific reference frame that also contains this orientational DT information. In this work, a population based DTI atlas has been developed that incorporates such orientational DT information with high accuracy and precision. The proposed methodology for constructing such an atlas is compared with a subject based DTI atlas, in which a single subject is selected as the reference image. Our results demonstrate that the population based atlas framework is more accurate with respect to the underlying diffusion information. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {{Van Hecke}, Wim and Sijbers, Jan and D'Agostino, Emiliano and Maes, Frederik and {De Backer}, Steve and Vandervliet, Evert and Parizel, Paul M. and Leemans, Alexander},
doi = {10.1016/j.neuroimage.2008.07.006},
file = {:Users/octavi/Documents/Mendeley Desktop/1-s2.0-S1053811908007970-main.pdf:pdf},
isbn = {1095-9572},
issn = {10538119},
journal = {NeuroImage},
keywords = {Diffusion tensor imaging,Diffusion tensor tractography,Inter-subject group atlas,Non-rigid coregistration},
number = {1},
pages = {69--80},
pmid = {18678261},
title = {{On the construction of an inter-subject diffusion tensor magnetic resonance atlas of the healthy human brain}},
volume = {43},
year = {2008}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:Users/octavi/Documents/Mendeley Desktop/Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2017}
}
@article{Hosang2016,
abstract = {Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detection performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.},
archivePrefix = {arXiv},
arxivId = {1502.05082},
author = {Hosang, Jan and Benenson, Rodrigo and Dollar, Piotr and Schiele, Bernt},
doi = {10.1109/TPAMI.2015.2465908},
eprint = {1502.05082},
file = {:Users/octavi/Documents/Mendeley Desktop/1502.05082.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer Vision,Detection proposals,object detection},
number = {4},
pages = {814--830},
pmid = {26959679},
title = {{What Makes for Effective Detection Proposals?}},
volume = {38},
year = {2016}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/octavi/Documents/Mendeley Desktop/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Ulyanov2017,
abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep{\_}image{\_}prior .},
archivePrefix = {arXiv},
arxivId = {1711.10925},
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
doi = {10.1109/CVPR.2017.623},
eprint = {1711.10925},
file = {:Users/octavi/Documents/Mendeley Desktop/deep{\_}image{\_}prior.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {2333-9403},
title = {{Deep Image Prior}},
url = {http://arxiv.org/abs/1711.10925},
year = {2017}
}
@article{Rhee,
author = {Rhee, Injong and Xu, Lisong},
file = {:Users/octavi/Documents/Mendeley Desktop/Rhee, Xu - Unknown - CUBIC A New TCP-Friendly High-Speed TCP Variant.pdf:pdf},
title = {{CUBIC: A New TCP-Friendly High-Speed TCP Variant}}
}
@article{Lo1995,
abstract = {We have developed a double-matching method and an artificial visual neural network technique for lung nodule detection. This neural network technique is generally applicable to the recognition of medical image pattern in gray scale imaging. The structure of the artificial neural net is a simplified network structure of human vision. The fundamental operation of the artificial neural network is local two-dimensional convolution rather than full connection with weighted multiplication. Weighting coefficients of the convolution kernels are formed by the neural network through backpropagated training. In addition, we modeled radiologists' reading procedures in order to instruct the artificial neural network to recognize the image patterns predefined and those of interest to experts in radiology. We have tested this method for lung nodule detection. The performance studies have shown the potential use of this technique in a clinical setting. This program first performed an initial nodule search with high sensitivity in detecting round objects using a sphere template double-matching technique. The artificial convolution neural network acted as a final classifier to determine whether the suspected image block contains a lung nodule. The total processing time for the automatic detection of lung nodules using both prescan and convolution neural network evaluation was about 15 seconds in a DEC Alpha workstation.},
annote = {Most important topic on the paper is the image segmentation process used, mimicking a radiologist.

Also, how the image is processed to normalize it and try to separate nodule from the rest of the masses that appear on it.

Interesting to note that they actually don't produce any sort of precision result? I should have better metrics, or something easier to calculate for my project.

Output metric kinda gives a likelihood, instead of a pure binary classification output.},
author = {Lo, Shih-Chung B and Lou, Shyh-Liang a and Lin, Jyh-Shyan and Freedman, Matthew T and Chien, Minze V and Mun, Seong K},
doi = {10.1109/42.476112},
file = {:Users/octavi/Documents/Mendeley Desktop/lo1995.pdf:pdf},
isbn = {0278-0062 (Print)$\backslash$n0278-0062 (Linking)},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
number = {4},
pages = {711--718},
pmid = {18215875},
title = {{Artificial convolution neural network techniques and applications for lung nodule detection}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=476112},
volume = {14},
year = {1995}
}
@misc{Ni,
author = {Nielsen, Jakob},
booktitle = {1997},
file = {:Users/octavi/Documents/Mendeley Desktop/Nielsen - Unknown - The Need for Speed.html:html},
keywords = {Internet connection,Web navigation,bandwidth,delay,download time,latency,modem,reponse time,speed},
title = {{The Need for Speed}},
url = {http://www.nngroup.com/articles/the-need-for-speed/},
urldate = {2013-05-18}
}
@article{Lin2017a,
abstract = {Convolutionalneuralnetworks (CNNs)havebeen applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have con- tributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications.We also introduce some of their current trends and remaining challenges.},
archivePrefix = {arXiv},
arxivId = {1706.02451},
author = {Lin, Tiger W. and Das, Anup and Krishnan, Giri P. and Bazhenov, Maxim and Sejnowski, Terrence J.},
doi = {10.1162/NECO},
eprint = {1706.02451},
file = {:Users/octavi/Documents/Mendeley Desktop/neco{\_}a{\_}00990.pdf:pdf},
isbn = {0899-7667},
issn = {1530888X},
pages = {2352--2449},
pmid = {25602775},
title = {{Deep Convolutional Neural Networks for Image Classification: AComprehensive Review}},
url = {http://arxiv.org/abs/1706.02451},
volume = {2449},
year = {2017}
}
@article{Brunner2015,
abstract = {The brain-computer interface (BCI) field has grown dramatically over the past few years, but there are still no coordinated efforts to ensure efficient communication and collaboration among key stakeholders. The European Commission (EC) has recently renewed their efforts to establish such a coordination effort by funding a coordination and support action for the BCI community called ‘BNCI Horizon 2020' after the ‘Future BNCI' project. Major goals of this new project include developing a roadmap for the next decade and beyond, encouraging discussion and collaboration within the BCI community, fostering communication with the general public, and the foundation of an international BCI Society. We present a short overview of current and past EU-funded BCI projects and provide evidence of a growing research and industrial community. Efficient communication also entails the establishment of clear terminology, which is a major goal of BNCI Horizon 2020. To this end, we give a brief overview of current BCI-related terms and definitions. A major networking activity in the project was the BNCI Horizon 2020 Retreat in Hallstatt, Austria. Over 60 experts participated in this event to discuss the future of the BCI field in a series of plenary talks, targeted discussions, and parallel focus sessions. A follow-up event was the EU BCI Day at the 6th International Brain-Computer Interface Conference in Graz, Austria. This networking event included plenary talks by eight companies and representatives from all seven ongoing EU research projects, poster presentations, demos, and discussions. Another goal of BNCI Horizon 2020 is the foundation of an official BCI Society. In this article, we summarize the current status of this process. Finally, we present visions for future BCI applications developed within BNCI Horizon 2020 using input from external BCI experts as well. We identify common themes and conclude with six exemplary use cases.},
author = {Brunner, Clemens and Birbaumer, Niels and Blankertz, Benjamin and Guger, Christoph and K{\"{u}}bler, Andrea and Mattia, Donatella and Mill{\'{a}}n, Jos{\'{e}} del R. and Miralles, Felip and Nijholt, Anton and Opisso, Eloy and Ramsey, Nick and Salomon, Patric and M{\"{u}}ller-Putz, Gernot R.},
doi = {10.1080/2326263X.2015.1008956},
file = {:Users/octavi/Documents/Mendeley Desktop/BNCI Horizon 2020 towards a roadmap for the BCI community.pdf:pdf},
isbn = {158113701X},
issn = {2326-263X},
journal = {Brain-Computer Interfaces},
keywords = {bci,collaboration,coordination,society},
number = {1},
pages = {1--10},
publisher = {Taylor {\&} Francis},
title = {{BNCI Horizon 2020: towards a roadmap for the BCI community}},
url = {http://www.tandfonline.com/doi/full/10.1080/2326263X.2015.1008956},
volume = {2},
year = {2015}
}
@article{Guger2009,
abstract = {An EEG-based brain-computer system can be used to control external devices such as computers, wheelchairs or Virtual Environments. One of the most important applications is a spelling device to aid severely disabled individuals with communication, for example people disabled by amyotrophic lateral sclerosis (ALS). P300-based BCI systems are optimal for spelling characters with high speed and accuracy, as compared to other BCI paradigms such as motor imagery. In this study, 100 subjects tested a P300-based BCI system to spell a 5-character word with only 5 min of training. EEG data were acquired while the subject looked at a 36-character matrix to spell the word WATER. Two different versions of the P300 speller were used: (i) the row/column speller (RC) that flashes an entire column or row of characters and (ii) a single character speller (SC) that flashes each character individually. The subjects were free to decide which version to test. Nineteen subjects opted to test both versions. The BCI system classifier was trained on the data collected for the word WATER. During the real-time phase of the experiment, the subject spelled the word LUCAS, and was provided with the classifier selection accuracy after each of the five letters. Additionally, subjects filled out a questionnaire about age, sex, education, sleep duration, working duration, cigarette consumption, coffee consumption, and level of disturbance that the flashing characters produced. 72.8{\%} (N = 81) of the subjects were able to spell with 100{\%} accuracy in the RC paradigm and 55.3{\%} (N = 38) of the subjects spelled with 100{\%} accuracy in the SC paradigm. Less than 3{\%} of the subjects did not spell any character correctly. People who slept less than 8 h performed significantly better than other subjects. Sex, education, working duration, and cigarette and coffee consumption were not statistically related to differences in accuracy. The disturbance of the flashing characters was rated with a median score of 1 on a scale from 1 to 5 (1, not disturbing; 5, highly disturbing). This study shows that high spelling accuracy can be achieved with the P300 BCI system using approximately 5 min of training data for a large number of non-disabled subjects, and that the RC paradigm is superior to the SC paradigm. 89{\%} of the 81 RC subjects were able to spell with accuracy 80-100{\%}. A similar study using a motor imagery BCI with 99 subjects showed that only 19{\%} of the subjects were able to achieve accuracy of 80-100{\%}. These large differences in accuracy suggest that with limited amounts of training data the P300-based BCI is superior to the motor imagery BCI. Overall, these results are very encouraging and a similar study should be conducted with subjects who have ALS to determine if their accuracy levels are similar. {\textcopyright} 2009 Elsevier Ireland Ltd.},
author = {Guger, Christoph and Daban, Shahab and Sellers, Eric and Holzner, Clemens and Krausz, Gunther and Carabalona, Roberta and Gramatica, Furio and Edlinger, Guenter},
doi = {10.1016/j.neulet.2009.06.045},
file = {:Users/octavi/Documents/Mendeley Desktop/NSL26188.pdf:pdf},
isbn = {1872-7972 (Electronic)},
issn = {03043940},
journal = {Neuroscience Letters},
keywords = {Brain-computer interface (BCI),Electroencephalogram (EEG),P300 potential,Rehabilitation,Spelling device},
number = {1},
pages = {94--98},
pmid = {19545601},
title = {{How many people are able to control a P300-based brain-computer interface (BCI)?}},
volume = {462},
year = {2009}
}
@article{Yu2016,
abstract = {Lung cancer is the most prevalent cancer worldwide, and histopathological assessment is indispensable for its diagnosis. However, human evaluation of pathology slides cannot accurately predict patients' prognoses. In this study, we obtain 2,186 haematoxylin and eosin stained histopathology whole-slide images of lung adenocarcinoma and squamous cell carcinoma patients from The Cancer Genome Atlas (TCGA), and 294 additional images from Stanford Tissue Microarray (TMA) Database. We extract 9,879 quantitative image features and use regularized machine-learning methods to select the top features and to distinguish shorter-term survivors from longer-term survivors with stage I adenocarcinoma (P{\textless}0.003) or squamous cell carcinoma (P=0.023) in the TCGA data set. We validate the survival prediction framework with the TMA cohort (P{\textless}0.036 for both tumour types). Our results suggest that automatically derived image features can predict the prognosis of lung cancer patients and thereby contribute to precision oncology. Our methods are extensible to histopathology images of other organs.},
author = {Yu, Kun-Hsing and Zhang, Ce and Berry, Gerald J. and Altman, Russ B. and R{\'{e}}, Christopher and Rubin, Daniel L. and Snyder, Michael},
doi = {10.1038/ncomms12474},
file = {:Users/octavi/Documents/Mendeley Desktop/yu2016.pdf:pdf},
isbn = {2041-1723},
issn = {2041-1723},
journal = {Nature Communications},
pages = {12474},
pmid = {27527408},
title = {{Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features}},
url = {http://www.nature.com/doifinder/10.1038/ncomms12474},
volume = {7},
year = {2016}
}
@article{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar},
chapter = {Boolean Re},
doi = {10.1109/LPT.2009.2020494},
editor = {Salas, A Cannon-Bowers E},
eprint = {0521865719 9780521865715},
institution = {Cambridge University Press Cambridge, England},
isbn = {0521865719},
issn = {13864564},
journal = {Online},
pmid = {10575050},
publisher = {Cambridge University Press},
title = {{An Introduction to Information Retrieval}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538},
year = {2009}
}
@article{Yang,
author = {Yang, Fan and Amer, Paul and Leighton, Jonathan and Belshe, Mike},
file = {:Users/octavi/Documents/Mendeley Desktop/Yang et al. - Unknown - A Methodology to Derive SPDY ' s Initial Dictionary for Zlib Compression.pdf:pdf},
keywords = {adaptive dictionary,compression,http header,spdy,zlib},
title = {{A Methodology to Derive SPDY ' s Initial Dictionary for Zlib Compression}},
url = {http://www.eecis.udel.edu/{~}amer/PEL/poc/pdf/SPDY-Fan.pdf}
}
@misc{Wu2017,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
booktitle = {Multimedia Tools and Applications},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
file = {:Users/octavi/Documents/Mendeley Desktop/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
pages = {1--17},
pmid = {23554596},
title = {{Deep residual learning for image recognition}},
year = {2017}
}
@book{Gourley2002,
author = {Gourley, David and Totty, Brian and Sayer, Marjorie and Reddy, Sailu and Aggarwal, Anshu},
file = {:Users/octavi/Documents/Mendeley Desktop/Gourley et al. - 2002 - HTTP The Definitive Guide.pdf:pdf},
isbn = {9781565925090},
pages = {658},
publisher = {O'Reilly},
title = {{HTTP The Definitive Guide}},
year = {2002}
}
@article{Kochanek2012,
abstract = {ObjectivesThis report presents final 2007 data on U.S. deaths, death rates, life expectancy, infant and maternal mortality, and trends by selected characteristics such as age, sex, Hispanic origin, race, marital status, educational attainment, injury at work, state of residence, and cause of death. MethodsInformation reported on death certificates, which are completed by funeral directors, attending physicians, medical exam- iners, and coroners, is presented in descriptive tabulations. The original records are filed in state registration offices. Statistical information is compiled in a national database through the Vital Statistics Cooperative Program of the Centers for Disease Control and Preventions National Center for Health Statistics. Causes of death are processed in accor- dance with the International Classification of Diseases, Tenth Revision. ResultsIn 2007, a total of 2,423,712 deaths were reported in the United States. The age-adjusted death rate was 760.2 deaths per 100,000 standard population, a decrease of 2.1 percent from the 2006 rate and a record low historical figure. Life expectancy at birth rose 0.2 year, from a 2006 value of 77.7 years to a record 77.9 in 2007. Age-specific death rates decreased for most age groups1524, 3544, 4554, 5564, 6574, 7584, and 85 and overand remained unchanged for the age groups of under age 1, 14, 514, and 2534. The 15 leading causes of death in 2007 remained the same as in 2006 with the exception of two causes that exchanged ranks. Alzheimers disease, the seventh leading cause of death in 2006, became the sixth leading cause in 2007, and Diabetes mellitus, the sixth leading cause in 2006, dropped to the seventh leading cause in 2007. Heart disease and cancer continued to be the leading and second-leading causes of death, respectively, together accounting for almost one-half of all deaths (48.6 percent). The infant mortality rate in 2007 was 6.75 deaths per 1,000 live births. ConclusionsMortality patterns in 2007, such as the decline in the age-adjusted death rate to a record historical low, were generally consistent with long-term trends. Life expectancy reached a record high in 2007, increasing 0.2 year from 2006.},
author = {Kochanek, Kenneth D and Xu, Jiaquan and Murphy, Sherry L and Minino, Arialdi M and Kung, Hsiang-Ching},
doi = {May 8, 2013},
file = {:Users/octavi/Documents/Mendeley Desktop/nvsr64{\_}02.pdf:pdf},
isbn = {1551-8922 (Print)$\backslash$n1551-8922 (Linking)},
issn = {15518922},
journal = {National Center for Health Statistics},
keywords = {c,cause death,death c life expectancy,life expectancy,mortality,mortality c cause,vital sta},
number = {3},
pages = {1--117},
pmid = {19788058},
title = {{National Vital Statistics Reports Deaths : Final Data for 2009}},
url = {http://www.cdc.gov/nchs/data/nvsr/nvsr58/nvsr58{\_}19.pdf},
volume = {60},
year = {2012}
}
@article{Singhal2017,
archivePrefix = {arXiv},
arxivId = {1712.07525},
author = {Singhal, Ayush and Sinha, Pradeep and Pant, Rakesh},
doi = {10.5120/ijca2017916055},
eprint = {1712.07525},
file = {:Users/octavi/Documents/Mendeley Desktop/dl-recommender-systems-survey-2005.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
keywords = {collaborative filtering,deep learning,hybrid system,literature review,machine learning,recommender system},
number = {7},
pages = {17--22},
title = {{Use of Deep Learning in Modern Recommendation System: A Summary of Recent Works}},
url = {http://www.ijcaonline.org/archives/volume180/number7/singhal-2017-ijca-916055.pdf},
volume = {180},
year = {2017}
}
@article{Kashani2018,
abstract = {Non-neovascular age-related macular degeneration (NNAMD) is a progressive blinding disease primarily due to loss of the retinal pigment epithelium (RPE) of the eye. Currently, there is no effective treatment for NNAMD. Now, Kashani and colleagues have developed a clinical-grade retinal implant made of human embryonic stem cell (hESC)–derived RPE grown on a synthetic substrate. In a first-in-human phase 1 clinical trial in five patients with advanced NNAMD, the implant was shown to be safe and well tolerated. Preliminary results reported potential therapeutic effects on visual acuity, suggesting that this approach might be useful for treating retinal disorders involving RPE loss.},
author = {Kashani, Amir H. and Lebkowski, Jane S. and Rahhal, Firas M. and Avery, Robert L. and Salehi-Had, Hani and Dang, Wei and Lin, Chih-Min and Mitra, Debbie and Zhu, Danhong and Thomas, Biju B. and Hikita, Sherry T. and Pennington, Britney O. and Johnson, Lincoln V. and Clegg, Dennis O. and Hinton, David R. and Humayun, Mark S.},
doi = {10.1126/SCITRANSLMED.AAO4097},
file = {:Users/octavi/Documents/Mendeley Desktop/10.1126@scitranslmed.aao4097.pdf:pdf},
issn = {1946-6234},
journal = {Science Translational Medicine},
number = {435},
pages = {eaao4097},
pmid = {29618560},
title = {{A bioengineered retinal pigment epithelial monolayer for advanced, dry age-related macular degeneration}},
url = {http://stm.sciencemag.org.sare.upf.edu/content/10/435/eaao4097/tab-figures-data},
volume = {10},
year = {2018}
}
@article{Herbert2009,
author = {Herbert, Tom and Dukkipati, Nandita and Refice, Tiziana and Cheng, Yuchung and Chu, Jerry and Sutin, Natalia and Agarwal, Amit and Jain, Arvind and Inc., Google},
file = {:Users/octavi/Documents/Mendeley Desktop/Herbert et al. - 2009 - An Argument for Increasing TCP ' s Initial Congestion Window.pdf:pdf},
keywords = {congestion control,internet measure-,tcp,web latency},
title = {{An Argument for Increasing TCP ' s Initial Congestion Window}},
year = {2009}
}
@article{VanGriethuysen2017,
author = {van Griethuysen, Joost J.M. and Fedorov, Andriy and Parmar, Chintan and Hosny, Ahmed and Aucoin, Nicole and Narayan, Vivek and Beets-Tan, Regina G.H. and Fillion-Robin, Jean-Christophe and Pieper, Steve and Aerts, Hugo J.W.L.},
doi = {10.1158/0008-5472.CAN-17-0339},
issn = {0008-5472},
journal = {Cancer Research},
month = {nov},
number = {21},
pages = {e104--e107},
title = {{Computational Radiomics System to Decode the Radiographic Phenotype}},
url = {http://cancerres.aacrjournals.org/lookup/doi/10.1158/0008-5472.CAN-17-0339},
volume = {77},
year = {2017}
}
@article{Muller-Putz2015a,
abstract = {The main objective of this roadmap is to provide a global perspective on the BCI field now and in the future. For readers not familiar with BCIs, we introduce basic terminology and concepts. We discuss what BCIs are, what BCIs can do, and who can benefit from BCIs. We illustrate our arguments with use cases to support the main messages. After reading this roadmap you will have a clear picture of the potential benefits and challenges of BCIs, the steps necessary to bridge the gap between current and future applications, and the potential impact of BCIs on society in the next decade and beyond.},
author = {Muller-Putz, G. R. and Brunner, Clemens and Bauernfeind, G. and Blefari, M. L. and Mill{\'{a}}n, Jos{\'{e}} del R. and Real, R. G. L. and K{\"{u}}bler, Andrea and Mattia, Donatella and Pichiorri, F. and Schettini, F. and Ramsey, Nick and H{\"{o}}hne, J. and Blankertz, Benjamin and Miralles, Felip and Otal, B. and Guger, Christoph and Ortner, Rupert and Poel, Mannes and Nijholt, Anton and Reuderink, Boris and Birbaumer, Niels and de Pobes, A. and Salomon, Patric and van Steensel, M. and Soekader, S. and Opisso, E.},
doi = {10.3390/s140814601.Allison},
file = {:Users/octavi/Documents/Mendeley Desktop/Appendix{\_}C{\_}End{\_}Users.pdf:pdf},
isbn = {9783851253795},
keywords = {Brain-Computer Interfaces},
pages = {1--48},
title = {{BNCI Horizon 2020 The Future of Brain / Neural Computer Interaction : Horizon 2020 Appendix A Research}},
url = {http://dx.doi.org/10.3217/978-3-85125-379-5{\%}5Cnhttp://bnci-horizon-2020.eu/images/bncih2020/Appendix{\_}A{\_}Research.pdf},
volume = {2020},
year = {2015}
}
@misc{Grigorik,
author = {Grigorik, Ilyia},
booktitle = {Marakana},
file = {:Users/octavi/Documents/Mendeley Desktop/Grigorik - 2012 - SPDY and the Road Towards HTTP 2.0.html:html},
keywords = {2.0,http,road,spdy,the,towards,training},
title = {{SPDY and the Road Towards HTTP 2.0}},
url = {http://marakana.com/s/post/1331/spdy{\_}http{\_}2{\_}0{\_}video},
urldate = {2013-05-17},
year = {2012}
}
@misc{W3.org,
author = {{World Wide Web Consortium (W3C)}},
file = {:Users/octavi/Documents/Mendeley Desktop/World Wide Web Consortium (W3C) - 1991 - The Original HTTP as defined in 1991.html:html},
title = {{The Original HTTP as defined in 1991}},
url = {http://www.w3.org/Protocols/HTTP/AsImplemented.html},
urldate = {2013-06-14},
year = {1991}
}
@article{Keane1999,
abstract = {Considerable evidence, accrued over the past decade, indicates that the presence of even relatively small amounts of protein or albumin in the urine is an important early sign of kidney disease and is a strong predictor of an increased risk for cardiovascular mortality and morbidity in certain high-risk groups within the general population. The early detection of proteinuria/albuminuria and institution of appropriate therapy has been shown to slow the progression of kidney disease. Similarly, the presence of abnormal quantities of protein in the urine identifies those who are at increased risk for myocardial infarction and stroke, and intensive management of such risk factors as high blood pressure and abnormal lipids may be of great benefit to these high-risk individuals. To make this information more available to the profession and the public, the National Kidney Foundation (NKF) convened a conference titled “Proteinuria, Albuminuria, Risk, Assessment, Detection and Elimination” (PARADE), in Nashville, Tennessee, on March 25 and 26, 1998. Three separate panels of experts and invited participants addressed issues related to proteinuria as a risk factor for cardiovascular disease, proteinuria as a mediator and marker of progressive kidney disease, and persistent massive proteinuria as the inciting factor that leads to the nephrotic syndrome. The initial expert recommendations that were developed for the evaluation and management of adults with proteinuria were presented at four Town Hall meetings in Minneapolis, Minnesota; San Antonio, Texas; Memphis, Tennessee; and Washington, DC, for additional input, comment, or further modification. These revised recommendations were submitted for review by the Scientific Advisory Board of the NKF and adopted as a position paper by the Board of Directors of the NKF at its meeting in Philadelphia on October 23, 1998. They will be submitted for review and comment to appropriate federal, medical, and public organizations before wider dissemination. In due course, a structured review of the subject will be necessary to develop guidelines. Specific recommendations for children and adolescents are in preparation.},
author = {Keane, W F and Eknoyan, G},
doi = {10.1016/S0272-6386(99)70442-7},
file = {:Users/octavi/Documents/Mendeley Desktop/keane1999.pdf:pdf},
issn = {1523-6838},
journal = {American journal of kidney diseases: the official journal of the National Kidney Foundation},
keywords = {Adult,Albuminuria,Cardiovascular Diseases,Cholesterol,Creatinine,Diet,Dietary Proteins,Humans,Hypertension,Kidney Diseases,Nephrotic Syndrome,Practice Guidelines as Topic,Proteinuria,Risk Factors,Serum Albumin,Sodium-Restricted},
number = {5},
pages = {1004--1010},
pmid = {10213663},
title = {{Proteinuria, albuminuria, risk, assessment, detection, elimination (PARADE): a position paper of the National Kidney Foundation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10213663},
volume = {33},
year = {1999}
}
@article{Leader2005,
author = {Leader, Joseph K. and Warfel, Thomas E. and Fuhrman, Carl R. and Golla, Sara K. and Weissfeld, Joel L. and Avila, Ricardo S. and Turner, Wesly D. and Zheng, Bin},
doi = {10.2214/AJR.04.1225},
file = {:Users/octavi/Documents/Mendeley Desktop/ajr.04.1225.pdf:pdf},
issn = {0361803X},
journal = {American Journal of Roentgenology},
number = {4},
pages = {973--978},
title = {{Pulmonary nodule detection with low-dose CT of the lung: Agreement among radiologists}},
volume = {185},
year = {2005}
}
@article{Wu2005,
abstract = {This paper presents two new strategies that can be used to greatly improve the speed of connected component labeling algorithms. To assign a label to a new object, most connected component labeling algorithms use a scanning step that examines some of its neighbors. The first strategy exploits the dependencies among them to reduce the number of neighbors examined. When considering 8-connected components in a 2D image, this can reduce the number of neighbors examined from four to one in many cases. The second strategy uses an array to store the equivalence information among the labels. This replaces the pointer based rooted trees used to store the same equivalence information. It reduces the memory required and also produces consecutive final labels. Using an array instead of the pointer based rooted trees speeds up the connected component labeling algorithms by a factor of 5 {\~{}}; 100 in our tests on random binary images.},
author = {Wu, Kesheng and Otoo, Ekow and Shoshani, Arie},
doi = {10.1117/12.596105},
file = {:Users/octavi/Documents/Mendeley Desktop/qt7jg5d1zn.pdf:pdf},
issn = {0277786X},
keywords = {connected component labeling,optimization,union-find},
pages = {1965},
title = {{Optimizing connected component labeling algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.596105},
year = {2005}
}
@misc{FCCa,
author = {FCC},
file = {:Users/octavi/Documents/Mendeley Desktop/FCC - 2011 - Measuring Broadband America - August 2011 FCC.gov.html:html},
title = {{Measuring Broadband America - August 2011 | FCC.gov}},
url = {http://www.fcc.gov/measuring-broadband-america/2011/august},
urldate = {2013-06-14},
year = {2011}
}
@article{Lv2017,
author = {Lv, Chunpeng and Yang, Yanmei and Jiang, Lixin and Gao, Lin and Rong, Shengzhong and Darko, Gottfried M. and Jiang, Wen and Gao, Yanhui and Sun, Dianjun},
doi = {10.1016/j.scitotenv.2017.07.101},
file = {:Users/octavi/Documents/Mendeley Desktop/lv2017.pdf:pdf},
issn = {00489697},
journal = {Science of The Total Environment},
keywords = {Incidence,Retrospective study,Sensitive diagnosis,Thyroid cancer,Water iodine},
pages = {735--741},
publisher = {Elsevier B.V.},
title = {{Association between chronic exposure to different water iodine and thyroid cancer: A retrospective study from 1995 to 2014}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0048969717318041},
volume = {609},
year = {2017}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
file = {:Users/octavi/Documents/Mendeley Desktop/1411.1792v1.pdf:pdf},
issn = {10495258},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
volume = {27},
year = {2014}
}
@article{Agarwal2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.04467v2},
author = {Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {arXiv:1603.04467v2},
file = {:Users/octavi/Documents/Mendeley Desktop/1603.04467.pdf:pdf},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
year = {2015}
}
@misc{M,
author = {Belshe, Mike},
file = {:Users/octavi/Documents/Mendeley Desktop/Belshe - Unknown - SPDY on Google servers - Google Groups.html:html},
title = {{SPDY on Google servers? - Google Groups}},
url = {https://groups.google.com/forum/?fromgroups{\#}!topic/spdy-dev/TCOW7Lw2scQ},
urldate = {2013-06-14}
}
@article{Rajpurkar2017,
abstract = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
archivePrefix = {arXiv},
arxivId = {1711.05225},
author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},
doi = {1711.05225},
eprint = {1711.05225},
file = {:Users/octavi/Documents/Mendeley Desktop/1711.05225.pdf:pdf},
pages = {3--9},
title = {{CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning}},
url = {http://arxiv.org/abs/1711.05225},
year = {2017}
}
@misc{RafaelRi,
author = {Rivera, Rafael},
file = {:Users/octavi/Documents/Mendeley Desktop/Rivera - 2013 - Blue's Clues Internet Explorer 11 getting SPDY support — Within Windows.html:html},
title = {{Blue's Clues: Internet Explorer 11 getting SPDY support — Within Windows}},
url = {http://withinwindows.com/within-windows/2013/4/1/blues-clues-internet-explorer-11-getting-spdy-support},
urldate = {2013-06-14},
year = {2013}
}
@article{Rohlfing2004,
abstract = {This paper evaluates strategies for atlas selection in atlas-based segmentation of three-dimensional biomedical images. Segmentation by intensity-based nonrigid registration to atlas images is applied to confocal microscopy images acquired from the brains of 20 bees. This paper evaluates and compares four different approaches for atlas image selection: registration to an individual atlas image (IND), registration to an average-shape atlas image (AVG), registration to the most similar image from a database of individual atlas images (SIM), and registration to all images from a database of individual atlas images with subsequent multi-classifier decision fusion (MUL). The MUL strategy is a novel application of multi-classifier techniques, which are common in pattern recognition, to atlas-based segmentation. For each atlas selection strategy, the segmentation performance of the algorithm was quantified by the similarity index (SI) between the automatic segmentation result and a manually generated gold standard. The best segmentation accuracy was achieved using the MUL paradigm, which resulted in a mean similarity index value between manual and automatic segmentation of 0.86 (AVG, 0.84; SIM, 0.82; IND, 0.81). The superiority of the MUL strategy over the other three methods is statistically significant (two-sided paired t test, P {\textless} 0.001). Both the MUL and AVG strategies performed better than the best possible SIM and IND strategies with optimal a posteriori atlas selection (mean similarity index for optimal SIM, 0.83; for optimal IND, 0.81). Our findings show that atlas selection is an important issue in atlas-based segmentation and that, in particular, multi-classifier techniques can substantially increase the segmentation accuracy. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Rohlfing, Torsten and Brandt, Robert and Menzel, Randolf and Maurer, Calvin R.},
doi = {10.1016/j.neuroimage.2003.11.010},
file = {:Users/octavi/Documents/Mendeley Desktop/rohlfing2004.pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
keywords = {Atlas selection,Atlas-based segmentation,Bee brain,Confocal microscopy imaging,Nonrigid image registration},
number = {4},
pages = {1428--1442},
pmid = {15050568},
title = {{Evaluation of atlas selection strategies for atlas-based image segmentation with application to confocal microscopy images of bee brains}},
volume = {21},
year = {2004}
}
@article{Bobadilla2013,
abstract = {Recommender systems have developed in parallel with the web. They were initially based on demographic, content-based and collaborative filtering. Currently, these systems are incorporating social information. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative filtering methods and algorithms; it also explains their evolution, provides an original classification for these systems, identifies areas of future implementation and develops certain areas selected for past, present or future importance. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bobadilla, J. and Ortega, F. and Hernando, A. and Guti{\'{e}}rrez, A.},
doi = {10.1016/j.knosys.2013.03.012},
eprint = {arXiv:1011.1669v3},
file = {:Users/octavi/Documents/Mendeley Desktop/recommender-systems-survey-2013.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Cold-start,Collaborative filtering,Evaluation metrics,Hybrid,Internet of things,Prediction,Recommendation,Recommender systems,Similarity measures,Social},
pages = {109--132},
pmid = {25246403},
publisher = {Elsevier B.V.},
title = {{Recommender systems survey}},
url = {http://dx.doi.org/10.1016/j.knosys.2013.03.012},
volume = {46},
year = {2013}
}
@book{Kernstine2010,
author = {Kernstine, K H and {Karen L. Reckamp}, M.D.M.S. and Thomas, C R},
isbn = {9781617050183},
publisher = {Springer Publishing Company},
series = {Current Multidisciplinary Oncology},
title = {{Lung Cancer: A Multidisciplinary Approach to Diagnosis and Management}},
url = {https://books.google.es/books?id=tq9vl31etFUC},
year = {2010}
}
@article{Radhakrishnan2011,
address = {New York, New York, USA},
author = {Radhakrishnan, Sivasankar and Cheng, Yuchung and Chu, Jerry and Jain, Arvind and Raghavan, Barath},
doi = {10.1145/2079296.2079317},
file = {:Users/octavi/Documents/Mendeley Desktop/Radhakrishnan et al. - 2011 - TCP fast open.pdf:pdf},
isbn = {9781450310413},
journal = {Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies on - CoNEXT '11},
keywords = {()},
pages = {1--12},
publisher = {ACM Press},
title = {{TCP fast open}},
url = {http://dl.acm.org/citation.cfm?doid=2079296.2079317},
year = {2011}
}
@book{Grigorik2013,
author = {Grigorik, Ilyia},
edition = {1st},
publisher = {O'Reilly},
title = {{High Performance Browser Networking}},
url = {http://chimera.labs.oreilly.com/books/1230000000545},
year = {2013}
}
@article{AmericanDiabetesAssociation2002,
author = {{American Diabetes Association}},
doi = {10.2337/diacare.25.2007.S33},
file = {:Users/octavi/Documents/Mendeley Desktop/Unknown - Unknown - Standards of Medical Care for Patients With Diabetes Mellitus.pdf:pdf},
issn = {0149-5992},
journal = {Diabetes care},
keywords = {Blood Glucose,Blood Glucose: analysis,Diabetes Mellitus,Diabetes Mellitus: diagnosis,Diabetes Mellitus: prevention {\&} control,Diabetes Mellitus: therapy,Health Care,Humans,Mass Screening,Mass Screening: standards,Quality Assurance,United States},
pages = {S33--49},
pmid = {12502618},
title = {{Standards of medical care for patients with diabetes mellitus.}},
url = {http://care.diabetesjournals.org/content/diacare/25/suppl{\_}1/s33.full.pdf http://www.ncbi.nlm.nih.gov/pubmed/12679266{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/12502618},
volume = {26 Suppl 1},
year = {2002}
}
@misc{FCC,
author = {FCC},
file = {:Users/octavi/Documents/Mendeley Desktop/FCC - 2013 - Measuring Broadband America - February 2013 FCC.gov.html:html},
title = {{Measuring Broadband America - February 2013 | FCC.gov}},
url = {http://www.fcc.gov/measuring-broadband-america/2013/February},
urldate = {2013-06-14},
year = {2013}
}
@article{Ghemawat2003,
abstract = {We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.},
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
doi = {10.1145/1165389.945450},
editor = {Roisin, C{\'{e}}cile and Munson, Ethan V and Vanoirbeek, Christine},
institution = {ACM},
isbn = {1581137575},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
keywords = {clustered storage,data storage,fault tolerance,scalability},
number = {5},
pages = {29},
pmid = {191},
publisher = {ACM},
series = {SOSP '03},
title = {{The Google file system}},
url = {http://portal.acm.org/citation.cfm?doid=1165389.945450},
volume = {37},
year = {2003}
}
@article{Kuan2017,
abstract = {We present a deep learning framework for computer-aided lung cancer diagnosis. Our multi-stage framework detects nodules in 3D lung CAT scans, determines if each nodule is malignant, and finally assigns a cancer probability based on these results. We discuss the challenges and advantages of our framework. In the Kaggle Data Science Bowl 2017, our framework ranked 41st out of 1972 teams.},
archivePrefix = {arXiv},
arxivId = {1705.09435},
author = {Kuan, Kingsley and Ravaut, Mathieu and Manek, Gaurav and Chen, Huiling and Lin, Jie and Nazir, Babar and Chen, Cen and Howe, Tse Chiang and Zeng, Zeng and Chandrasekhar, Vijay},
eprint = {1705.09435},
file = {:Users/octavi/Documents/Mendeley Desktop/1705.09435.pdf:pdf},
keywords = {3d,deep learning,lung cancer,neural networks,nodule detection},
title = {{Deep Learning for Lung Cancer Detection: Tackling the Kaggle Data Science Bowl 2017 Challenge}},
url = {http://arxiv.org/abs/1705.09435},
year = {2017}
}
@article{He2014,
archivePrefix = {arXiv},
arxivId = {1502.01852v1},
author = {He, Kaiming},
doi = {10.1.1.725.4861},
eprint = {1502.01852v1},
file = {:Users/octavi/Documents/Mendeley Desktop/He{\_}Delving{\_}Deep{\_}into{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pmid = {7410480},
title = {{Delving Deep into Rectifiers : Surpassing Human-Level Performance on ImageNet Classification}},
year = {2014}
}
@article{Belshea,
author = {Belshe, Mike and Peon, Roberto},
title = {{SPDY Protocol}},
url = {http://tools.ietf.org/html/draft-mbelshe-httpbis-spdy-00}
}
@article{Chaudhary2016,
abstract = {Functional near-infrared spectroscopy (NIRS) and functional magnetic resonance imaging (fMRI) are non-invasive methods for acquiring hemodynamic signals from the brain with the primary benefit of anatomical specificity of signals. Recently, there has been a surge of studies with NIRS and fMRI for the implementation of a brain-computer interface (BCI), for the acquisition, decoding and regulation of hemodynamic signals in the brain, and to investigate their behavioural consequences. Both NIRS and fMRI rely on the measurement of the task-induced blood oxygen level-dependent response. In this review, we consider fundamental principles, recent developments, applications and future directions and challenges of NIRS-based and fMRI-based BCIs. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {10.1016/j.neunet.2009.05.009},
author = {Chaudhary, Ujwal and Birbaumer, Niels and Ramos-Murguialday, Ander},
doi = {10.1038/nrneurol.2016.113},
eprint = {j.neunet.2009.05.009},
file = {:Users/octavi/Documents/Mendeley Desktop/nrneurol.2016.113.pdf:pdf},
isbn = {0893-6080},
issn = {17594766},
journal = {Nature Reviews Neurology},
number = {9},
pages = {513--525},
pmid = {27539560},
primaryClass = {10.1016},
title = {{Brain-computer interfaces for communication and rehabilitation}},
volume = {12},
year = {2016}
}
@book{BNCIHorizon20202015,
abstract = {Since their inception in the early 1970s, brain-computer interfaces have been a fascinating topic, both for sci- entists and science ction writers alike. Right now, BCIs are on the verge of evolving from lab prototypes into useful real-world products. Several applications have already been implemented successfully, e.g. visual P300 spellers that establish a new communication channel in people with severe disability. However, the road to future BCI products needs to be clari ed. While the eld has progressed rapidly from its infancy, a concerted e ort supported by all stakeholders is necessary to make further progress toward future BCI applications. Researchers are ready to address the speci c needs of potential end users together with companies to produce BCI solutions that can be applied outside of laboratory settings. In this roadmap, we go beyond the current state of the art of BCI research and outline real-life applications, future scenarios, and major challenges. We analyze the market for future BCI products and discuss societal and ethical issues. We conclude with general recommendations on future developments and funding opportuni- ties to support a continued and sustainable growth of the BCI eld. This roadmap adheres to a very concise format, in which we condense the most important information for decision makers and policy makers alike. For those interested in a more detailed treatment of the presented topics, we compiled more in-depth information in ve appendices available on our website.},
author = {{BNCI Horizon 2020}},
booktitle = {The Future BNCI Project},
doi = {10.3217/978-3-85125-379-5},
file = {:Users/octavi/Documents/Mendeley Desktop/Roadmap{\_}BNCI{\_}Horizon{\_}2020.pdf:pdf},
isbn = {9783851253795},
pages = {48},
title = {{Roadmap: The Future in Brain/Neural-Computer Interaction: Horizon 2020}},
url = {http://bnci-horizon-2020.eu/},
year = {2015}
}
@article{Zhu,
abstract = {In this work, we present a fully automated lung CT cancer diagnosis system, DeepLung. DeepLung contains two parts, nodule detection and classification. Considering the 3D na-ture of lung CT data, two 3D networks are designed for the nodule detection and classification respectively. Specifically, a 3D Faster R-CNN is designed for nodule detection with a U-net-like encoder-decoder structure to effectively learn nod-ule features. For nodule classification, gradient boosting ma-chine (GBM) with 3D dual path network (DPN) features is proposed. The nodule classification subnetwork is validated on a public dataset from LIDC-IDRI, on which it achieves better performance than state-of-the-art approaches, and sur-passes the average performance of four experienced doctors. For the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagno-sis is conducted by the classification subnetwork. Extensive experimental results demonstrate the DeepLung is compara-ble to the experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.},
author = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
file = {:Users/octavi/Documents/Mendeley Desktop/Zhu et al. - Unknown - DeepLung 3D Deep Convolutional Nets for Automated Pulmonary Nodule Detection and Classification.pdf:pdf},
title = {{DeepLung: 3D Deep Convolutional Nets for Automated Pulmonary Nodule Detection and Classification}},
url = {https://arxiv.org/pdf/1709.05538.pdf}
}
@misc{Fielding,
author = {Fielding, R. and Irvine, UC and Gettys, J. and Mogul, J. and DEC and Frystyk, H. and Berners-Lee, T. and MIT/LCS},
file = {:Users/octavi/Documents/Mendeley Desktop/Fielding et al. - 1997 - RFC2068 Hypertext Transfer Protocol -- HTTP1.1 memo.html:html},
title = {{RFC2068 Hypertext Transfer Protocol -- HTTP/1.1 memo}},
url = {http://www.ietf.org/rfc/rfc2068.txt},
urldate = {2013-06-14},
year = {1997}
}
@misc{FCC20,
author = {FCC},
file = {:Users/octavi/Documents/Mendeley Desktop/FCC - 2012 - Measuring Broadband America - July 2012 FCC.gov.html:html},
title = {{Measuring Broadband America - July 2012 | FCC.gov}},
url = {http://www.fcc.gov/measuring-broadband-america/2012/july},
urldate = {2013-06-14},
year = {2012}
}
@article{Siegel2017,
abstract = {Each year, the American Cancer Society estimates the numbers of new can-cer cases and deaths that will occur in the United States in the current year and com-piles the most recent data on cancer incidence, mortality, and survival. Incidence data were collected by the Surveillance, Epidemiology, and End Results Program; the National Program of Cancer Registries; and the North American Association of Central Cancer Registries. Mortality data were collected by the National Center for Health Statistics. In 2017, 1,688,780 new cancer cases and 600,920 cancer deaths are projected to occur in the United States. For all sites combined, the cancer incidence rate is 20{\%} higher in men than in women, while the cancer death rate is 40{\%} higher. However, sex disparities vary by cancer type. For example, thyroid cancer incidence rates are 3-fold higher in women than in men (21 vs 7 per 100,000 population), despite equivalent death rates (0.5 per 100,000 population), largely reflecting sex differences in the " epidemic of diag-nosis. " Over the past decade of available data, the overall cancer incidence rate (2004-2013) was stable in women and declined by approximately 2{\%} annually in men, while the cancer death rate (2005-2014) declined by about 1.5{\%} annually in both men and women. From 1991 to 2014, the overall cancer death rate dropped 25{\%}, translating to approxi-mately 2,143,200 fewer cancer deaths than would have been expected if death rates had remained at their peak. Although the cancer death rate was 15{\%} higher in blacks than in whites in 2014, increasing access to care as a result of the Patient Protection and Affordable Care Act may expedite the narrowing racial gap; from 2010 to 2015, the proportion of blacks who were uninsured halved, from 21{\%} to 11{\%}, as it did for Hispanics (31{\%} to 16{\%}). Gains in coverage for traditionally underserved Americans will facilitate the broader application of existing cancer control knowledge across every segment of the population.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Siegel, Rebecca L. and Miller, Kimberly D. and Jemal, Ahmedin},
doi = {10.3322/caac.21387},
eprint = {arXiv:1011.1669v3},
file = {:Users/octavi/Documents/Mendeley Desktop/Siegel{\_}et{\_}al-2017-CA{\_}{\_}A{\_}Cancer{\_}Journal{\_}for{\_}Clinicians.pdf:pdf},
isbn = {1542-4863 (Electronic)$\backslash$r0007-9235 (Linking)},
issn = {00079235},
journal = {CA: A Cancer Journal for Clinicians},
keywords = {cancer cases,cancer statistics,death rates,incidence,mortality},
number = {1},
pages = {7--30},
pmid = {25559415},
title = {{Cancer statistics, 2017}},
url = {http://doi.wiley.com/10.3322/caac.21387},
volume = {67},
year = {2017}
}
@book{Allspaw,
author = {Allspaw, John},
file = {:Users/octavi/Documents/Mendeley Desktop/Allspaw - 2008 - The Art of Capacity Planning.pdf:pdf},
isbn = {9780596518578},
pages = {154},
publisher = {O'Reilly},
title = {{The Art of Capacity Planning}},
year = {2008}
}
@article{Mirashe2010,
abstract = {Computing as you know it is about to change, your applications and documents are going to move from the desktop into the cloud. I'm talking about cloud computing, where applications and files are hosted on a "cloud" consisting of thousands of computers and servers, all linked together and accessible via the Internet. With cloud computing, everything you do is now web based instead of being desktop based. You can access all your programs and documents from any computer that's connected to the Internet. How will cloud computing change the way you work? For one thing, you're no longer tied to a single computer. You can take your work anywhere because it's always accessible via the web. In addition, cloud computing facilitates group collaboration, as all group members can access the same programs and documents from wherever they happen to be located. Cloud computing might sound far-fetched, but chances are you're already using some cloud applications. If you're using a web-based email program, such as Gmail or Hotmail, you're computing in the cloud. If you're using a web-based application such as Google Calendar or Apple Mobile Me, you're computing in the cloud. If you're using a file- or photo-sharing site, such as Flickr or Picasa Web Albums, you're computing in the cloud. It's the technology of the future, available to use today.},
author = {Mirashe, Shivaji P and Kalyankar, N V},
chapter = {6},
editor = {Antonopoulos, Nick and Gillam, Lee},
file = {:Users/octavi/Documents/Mendeley Desktop/Mirashe, Kalyankar - 2010 - Cloud Computing.pdf:pdf},
institution = {Universit{\{}{\"{a}}{\}}t Stuttgart, Fakult{\{}{\"{a}}{\}}t Informatik, Elektrotechnik und Informationstechnik, Germany},
journal = {Communications of the ACM},
number = {7},
pages = {9},
publisher = {ACM},
series = {Informatik im Fokus},
title = {{Cloud Computing}},
url = {http://arxiv.org/abs/1003.4074},
volume = {51},
year = {2010}
}
@article{Chu,
author = {Chu, Jerry and Jain, Arvind and Cheng, Yuchung and Radhakrishnan, Sivasankar},
title = {{TCP Fast Open RFC draft}},
url = {http://tools.ietf.org/html/draft-ietf-tcpm-fastopen-03}
}
@article{Rebsamen2006,
abstract = {This paper presents the first working prototype of a brain controlled wheelchair able to navigate inside a typical office or hospital environment. This brain controlled wheelchair (BCW) is based on a slow but safe P300 interface. To circumvent the problem caused by the low information rate of the EEG signal, we propose a motion guidance strategy providing safe and efficient control without complex sensors or sensor processing. Experiments demonstrated that healthy subjects could safely control the wheelchair in an office like environment, without any training},
author = {Rebsamen, Brice and Burdet, Etienne and Guan, Cuntai and Zhang, Haihong and Teo, Chee Leong and Zeng, Qiang and Ang, Marcelo and Laugier, Christian},
doi = {10.1109/BIOROB.2006.1639239},
file = {:Users/octavi/Documents/Mendeley Desktop/01639239.pdf:pdf},
isbn = {1424400406},
journal = {Proceedings of the First IEEE/RAS-EMBS International Conference on Biomedical Robotics and Biomechatronics, 2006, BioRob 2006},
keywords = {BCI,P300,Path following,Wheelchair},
pages = {1101--1106},
title = {{A brain-controlled wheelchair based on P300 and path guidance}},
volume = {2006},
year = {2006}
}
@article{Hernandez-Leo2011,
author = {Hern{\'{a}}ndez-Leo, Davinia and Romeo, Lauren and Carralero, Miguel a. and Chac{\'{o}}n, Jonathan and Carri{\'{o}}, Mar and Moreno, Pau and Blat, Josep},
doi = {10.1016/j.compedu.2011.06.016},
file = {:Users/octavi/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Leo et al. - 2011 - LdShake Learning design solutions sharing and co-edition.pdf:pdf},
issn = {03601315},
journal = {Computers {\&} Education},
keywords = {authoring tools and methods,computer-mediated communication,human-computer interface},
month = {dec},
number = {4},
pages = {2249--2260},
publisher = {Elsevier Ltd},
title = {{LdShake: Learning design solutions sharing and co-edition}},
url = {http://ldshake.upf.edu},
volume = {57},
year = {2011}
}
@article{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus Robert},
doi = {10.1007/978-3-642-35289-8-3},
eprint = {9809069v1},
file = {:Users/octavi/Documents/Mendeley Desktop/lecun-98b.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {9--48},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Efficient backprop}},
volume = {7700 LECTU},
year = {2012}
}
@article{Tan2006,
author = {Tan, K. and Song, J. and Zhang, Q. and Sridharan, M.},
doi = {10.1109/INFOCOM.2006.188},
file = {:Users/octavi/Documents/Mendeley Desktop/Tan et al. - 2006 - A Compound TCP Approach for High-Speed and Long Distance Networks.pdf:pdf},
isbn = {1-4244-0221-2},
journal = {Proceedings IEEE INFOCOM 2006. 25TH IEEE International Conference on Computer Communications},
pages = {1--12},
publisher = {Ieee},
title = {{A Compound TCP Approach for High-Speed and Long Distance Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4146841},
year = {2006}
}
@article{Nicolas-Alonso2012,
abstract = {A brain-computer interface (BCI) is a hardware and software communications system that permits cerebral activity alone to control computers or external devices. The immediate goal of BCI research is to provide communications capabilities to severely disabled people who are totally paralyzed or 'locked in' by neurological neuromuscular disorders, such as amyotrophic lateral sclerosis, brain stem stroke, or spinal cord injury. Here, we review the state-of-the-art of BCIs, looking at the different steps that form a standard BCI: signal acquisition, preprocessing or signal enhancement, feature extraction, classification and the control interface. We discuss their advantages, drawbacks, and latest advances, and we survey the numerous technologies reported in the scientific literature to design each step of a BCI. First, the review examines the neuroimaging modalities used in the signal acquisition step, each of which monitors a different functional brain activity such as electrical, magnetic or metabolic activity. Second, the review discusses different electrophysiological control signals that determine user intentions, which can be detected in brain activity. Third, the review includes some techniques used in the signal enhancement step to deal with the artifacts in the control signals and improve the performance. Fourth, the review studies some mathematic algorithms used in the feature extraction and classification steps which translate the information in the control signals into commands that operate a computer or other device. Finally, the review provides an overview of various BCI applications that control a range of devices.},
archivePrefix = {arXiv},
arxivId = {10.3390/s120201211},
author = {Nicolas-Alonso, Luis Fernando and Gomez-Gil, Jaime},
doi = {10.3390/s120201211},
eprint = {s120201211},
file = {:Users/octavi/Documents/Mendeley Desktop/sensors-12-01211.pdf:pdf},
isbn = {0308018032250},
issn = {14248220},
journal = {Sensors},
keywords = {Artifact,Brain-computer interface (BCI),Brain-machine interface,Collaborative sensor system,Electroencephalography (EEG),Neuroimaging,Rehabilitation},
number = {2},
pages = {1211--1279},
pmid = {22438708},
primaryClass = {10.3390},
title = {{Brain computer interfaces, a review}},
volume = {12},
year = {2012}
}
@article{Stevens,
author = {Stevens, W. Richard and Allman, Mark and Paxson, Vern},
title = {{TCP Congestion Control}},
url = {https://tools.ietf.org/html/rfc2581},
year = {1999}
}
@article{VanRikxoort2009,
abstract = {Lung segmentation is a prerequisite for automated analysis of chest CT scans. Conventional lung segmentation methods rely on large attenuation differences between lung parenchyma and surrounding tissue. These methods fail in scans where dense abnormalities are present, which often occurs in clinical data. Some methods to handle these situations have been proposed, but they are too time consuming or too specialized to be used in clinical practice. In this article, a new hybrid lung segmentation method is presented that automatically detects failures of a conventional algorithm and, when needed, resorts to a more complex algorithm, which is expected to produce better results in abnormal cases. In a large quantitative evaluation on a database of 150 scans from different sources, the hybrid method is shown to perform substantially better than a conventional approach at a relatively low increase in computational cost.},
author = {van Rikxoort, Eva M. and de Hoop, Bartjan and Viergever, Max A. and Prokop, Mathias and van Ginneken, Bram},
doi = {10.1118/1.3147146},
file = {:Users/octavi/Documents/Mendeley Desktop/Rikxoort{\_}et{\_}al-2009-Medical{\_}Physics.pdf:pdf},
isbn = {0094-2405 (Print)$\backslash$n0094-2405 (Linking)},
issn = {00942405},
journal = {Medical Physics},
keywords = {automatic,ct,error detection,lung,segmentation},
number = {7},
pages = {2934--2947},
pmid = {19673192},
title = {{Automatic lung segmentation from thoracic computed tomography scans using a hybrid approach with error detection}},
url = {http://doi.wiley.com/10.1118/1.3147146},
volume = {36},
year = {2009}
}
@article{An,
annote = {1st on LUNA16 leaderboard},
author = {An, Ping and Shenzhen, Technology},
file = {:Users/octavi/Documents/Mendeley Desktop/20171220{\_}083208{\_}PAtech{\_}FPRED.pdf:pdf},
title = {{3DCNN for Lung Nodule Detection And False Positive Reduction 2 Data Preprocessing}}
}
@misc{Hoffman,
author = {Hoffman, Billy},
file = {:Users/octavi/Documents/Mendeley Desktop/Hoffman - Unknown - Explaining the CRIME weakness in SPDY and SSL Zoompf.html:html},
title = {{Explaining the CRIME weakness in SPDY and SSL | Zoompf}},
url = {http://zoompf.com/2012/09/explaining-the-crime-weakness-in-spdy-and-ssl},
urldate = {2013-06-14}
}
@article{Lin2017,
abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that {\$}n{\$} variables cannot be multiplied using fewer than 2{\^{}}n neurons in a single hidden layer.},
archivePrefix = {arXiv},
arxivId = {1608.08225},
author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
doi = {10.1007/s10955-017-1836-5},
eprint = {1608.08225},
file = {:Users/octavi/Documents/Mendeley Desktop/why{\_}deep{\_}learning{\_}works.pdf:pdf},
isbn = {9781627480031},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Artificial neural networks,Deep learning,Statistical physics},
number = {6},
pages = {1223--1247},
pmid = {25246403},
title = {{Why Does Deep and Cheap Learning Work So Well?}},
volume = {168},
year = {2017}
}
@book{Miner2012,
author = {Miner, Donald and Shook, Adam},
edition = {1st},
file = {:Users/octavi/Documents/Mendeley Desktop/Miner, Shook - 2012 - MapReduce Design Patterns.pdf:pdf},
isbn = {9781449327170},
pages = {251},
publisher = {O'Reilly},
title = {{MapReduce Design Patterns}},
year = {2012}
}
@misc{Raghav,
author = {Raghav, Atul},
file = {:Users/octavi/Documents/Mendeley Desktop/Raghav - 2012 - Twitter Adopts SPDY.html:html},
title = {{Twitter Adopts SPDY}},
url = {http://browserfame.com/527/twitter-spdy-support},
urldate = {2013-06-14},
year = {2012}
}
@misc{Grigorika,
author = {Grigorik, Ilyia},
file = {:Users/octavi/Documents/Mendeley Desktop/Grigorik - 2013 - High Performance Networking in Google Chrome - igvita.com.html:html},
title = {{High Performance Networking in Google Chrome - igvita.com}},
url = {http://www.igvita.com/posa/high-performance-networking-in-google-chrome/{\#}tcp-pre-connect},
urldate = {2013-06-14},
year = {2013}
}
@article{Baylor2017,
abstract = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful or-chestration of many components—a learner for generating models based on training data, modules for analyzing and val-idating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such or-chestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the compo-nents, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2{\%} increase in app installs resulting from improved data and model analysis.},
author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and {Yu Foo}, Chuan and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and {Yuen Koo}, Chiu and Lew, Lukasz and Mewald, Clemens and {Naresh Modi}, Akshay and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and {Euijong Whang}, Steven and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and {Zinkevich Google Inc}, Martin},
doi = {10.1145/3097983.3098021},
file = {:Users/octavi/Documents/Mendeley Desktop/tfx{\_}paper.pdf:pdf},
isbn = {9781450348874},
journal = {Kdd},
keywords = {continuous training,end-to-end platform,large-scale machine learning},
pages = {1387--1395},
title = {{TFX: A TensorFlow-Based Production-Scale Machine Learning Platform}},
url = {http://delivery.acm.org/10.1145/3100000/3098021/p1387-baylor.pdf?ip=149.132.25.65{\&}id=3098021{\&}acc=OPENTOC{\&}key=296E2ED678667973.B9B72D0607302676.4D4702B0C3E38B35.054E54E275136550{\&}CFID=986587823{\&}CFTOKEN=28999827{\&}{\_}{\_}acm{\_}{\_}=1507019813{\_}ab7be4a96be057630413a29a111},
year = {2017}
}
@article{Sussillo2016,
abstract = {A major hurdle to clinical translation of brain-machine interfaces (BMIs) is that current decoders, which are trained from a small quantity of recent data, become ineffective when neural recording conditions subsequently change. We tested whether a decoder could be made more robust to future neural variability by training it to handle a variety of recording conditions sampled from months of previously collected data as well as synthetic training data perturbations. We developed a new multiplicative recurrent neural network BMI decoder that successfully learned a large variety of neural-to- kinematic mappings and became more robust with larger training datasets. When tested with a non-human primate preclinical BMI model, this decoder was robust under conditions that disabled a state-of-the-art Kalman filter based decoder. These results validate a new BMI strategy in which accumulated data history is effectively harnessed, and may facilitate reliable daily BMI use by reducing decoder retraining downtime.},
archivePrefix = {arXiv},
arxivId = {1610.05872},
author = {Sussillo, David and Stavisky, Sergey D. and Kao, Jonathan C. and Ryu, Stephen I. and Shenoy, Krishna V.},
doi = {10.1038/ncomms13749},
eprint = {1610.05872},
file = {:Users/octavi/Documents/Mendeley Desktop/ncomms13749.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
pages = {1--12},
pmid = {27958268},
publisher = {Nature Publishing Group},
title = {{Making brain-machine interfaces robust to future neural variability}},
url = {http://dx.doi.org/10.1038/ncomms13749},
volume = {7},
year = {2016}
}
@book{Souders2007,
author = {Souders, Steve},
file = {:Users/octavi/Documents/Mendeley Desktop/Souders - 2007 - High Performance Web Sites.pdf:pdf},
isbn = {9780596529307},
pages = {170},
publisher = {O'Reilly},
title = {{High Performance Web Sites}},
year = {2007}
}
@misc{Be,
author = {Belshe, Mike and Peon, Roberto},
file = {:Users/octavi/Documents/Mendeley Desktop/Belshe, Peon - Unknown - Chromium Blog A 2x Faster Web.html:html},
keywords = {Mike Belshe,Roberto Peon},
title = {{Chromium Blog: A 2x Faster Web}},
url = {http://blog.chromium.org/2009/11/2x-faster-web.html},
urldate = {2013-06-13}
}
@article{LuoLi,
abstract = {—Good software engineers are essential to the creation of good software. However, most of what we know about software-engineering expertise are vague stereotypes, such as 'excellent communicators' and 'great teammates'. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.},
author = {{Luo Li}, Paul and Ko, Andrew J and Zhu, Jiamin},
file = {:Users/octavi/Documents/Mendeley Desktop/Li2015GreatEngineers.pdf:pdf},
keywords = {Index Terms—Software engineers,expertise,teamwork},
title = {{What Makes A Great Software Engineer?}},
url = {https://faculty.washington.edu/ajko/papers/Li2015GreatEngineers.pdf}
}
@article{Fiorio1996,
abstract = {We consider Union-Find as an appropriate data structure to obtain two linear time algorithms for the segmentation of images. The linearity is obtained by restricting the order in which Union's are performed. For one algorithm the complexity bound is proven by amortizing the Find operations. For the other we use periodic updates to keep the relevant part of our Union-Find-tree of constant height. Both algorithms are generalized and lead to new linear strategies for Union-Find that are neither covered by the algorithm of Gabow and Tarjan (1984) nor by the one of Dillencourt et al. (1992).},
author = {Fiorio, Christophe and Gustedt, Jens},
doi = {10.1016/0304-3975(94)00262-2},
file = {:Users/octavi/Documents/Mendeley Desktop/1-s2.0-0304397594002622-main.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
number = {2},
pages = {165--181},
title = {{Two linear time Union-Find strategies for image processing}},
volume = {154},
year = {1996}
}
@article{Muller-Putz2015c,
abstract = {The main objective of this roadmap is to provide a global perspective on the BCI field now and in the future. For readers not familiar with BCIs, we introduce basic terminology and concepts. We discuss what BCIs are, what BCIs can do, and who can benefit from BCIs. We illustrate our arguments with use cases to support the main messages. After reading this roadmap you will have a clear picture of the potential benefits and challenges of BCIs, the steps necessary to bridge the gap between current and future applications, and the potential impact of BCIs on society in the next decade and beyond.},
author = {Muller-Putz, G. R. and Brunner, Clemens and Bauernfeind, G. and Blefari, M. L. and Mill{\'{a}}n, Jos{\'{e}} del R. and Real, R. G. L. and K{\"{u}}bler, Andrea and Mattia, Donatella and Pichiorri, F. and Schettini, F. and Ramsey, Nick and H{\"{o}}hne, J. and Blankertz, Benjamin and Miralles, Felip and Otal, B. and Guger, Christoph and Ortner, Rupert and Poel, Mannes and Nijholt, Anton and Reuderink, Boris and Birbaumer, Niels and de Pobes, A. and Salomon, Patric and van Steensel, M. and Soekader, S. and Opisso, E.},
doi = {10.3390/s140814601.Allison},
file = {:Users/octavi/Documents/Mendeley Desktop/References{\_}Appendix.pdf:pdf},
isbn = {9783851253795},
keywords = {Brain-Computer Interfaces},
pages = {1--48},
title = {{BNCI Horizon 2020 The Future of Brain / Neural Computer Interaction : Horizon 2020 Appendix A Research}},
url = {http://dx.doi.org/10.3217/978-3-85125-379-5{\%}5Cnhttp://bnci-horizon-2020.eu/images/bncih2020/Appendix{\_}A{\_}Research.pdf},
volume = {2020},
year = {2015}
}
@article{Team2011,
abstract = {The aggressive and heterogeneous nature of lung cancer has thwarted efforts to reduce mortality from this cancer through the use of screening. The advent of low- dose helical computed tomography (CT) altered the landscape of lung-cancer screen- ing, with studies indicating that low-dose CT detects many tumors at early stages. The National Lung Screening Trial (NLST) was conducted to determine whether screening with low-dose CT could reduce mortality from lung cancer.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Team, The National Lung Screening Trial Research},
doi = {10.1056/NEJMoa1411087},
eprint = {arXiv:1011.1669v3},
file = {:Users/octavi/Documents/Mendeley Desktop/nejmoa1102873.pdf:pdf},
isbn = {1533-4406 (Electronic) 0028-4793 (Linking)},
issn = {0028-4793},
journal = {New England Journal of Medicine},
number = {5},
pages = {395--409},
pmid = {25482239},
title = {{Reduced Lung-Cancer Mortality with Low-Dose Computed Tomographic Screening}},
volume = {365},
year = {2011}
}
@article{Kula2015,
abstract = {I present a hybrid matrix factorisation model representing users and items as linear combinations of their content features' latent factors. The model outperforms both collaborative and content-based models in cold-start or sparse interaction data scenarios (using both user and item metadata), and performs at least as well as a pure collaborative matrix factorisation model where interaction data is abundant. Additionally, feature embeddings produced by the model encode semantic information in a way reminiscent of word embedding approaches, making them useful for a range of related tasks such as tag recommendations.},
archivePrefix = {arXiv},
arxivId = {1507.08439},
author = {Kula, Maciej},
eprint = {1507.08439},
file = {:Users/octavi/Documents/Mendeley Desktop/paper4.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Cold-start,Matrix Factorization,Recommender Systems},
pages = {14--21},
title = {{Metadata embeddings for user and item cold-start recommendations}},
volume = {1448},
year = {2015}
}
@article{Arindra2017,
abstract = {Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objec-tive evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95{\%} at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.08012v4},
author = {Arindra, Arnaud and Setio, Adiyoso and Traverso, Alberto and {De Bel}, Thomas and Berens, Moira S N and {Van Den Bogaard}, Cas and Cerello, Piergiorgio and Chen, Hao and Dou, Qi and Fantacci, Maria Evelina and Geurts, Bram and {Van Der Gugten}, Robbert and Heng, Ann and Jansen, Bart and {De Kaste}, Michael M J and Kotov, Valentin and Yu, Jack and Lin, -Hung and Manders, Jeroen T M C and S{\'{o}}{\~{n}}ora-Mengana, Alexander and Garc{\'{i}}a-Naranjo, Juan Carlos and Papavasileiou, Evgenia and Prokop, Mathias and Saletta, Marco and Schaefer-Prokop, Cornelia M and Scholten, Ernst T and Scholten, Luuk and Snoeren, Miranda M and Torres, Ernesto Lopez and Vandemeulebroucke, Jef and Walasek, Nicole and Zuidhof, Guido C A and {Van Ginneken}, Bram and Jacobs, Colin},
eprint = {arXiv:1612.08012v4},
file = {:Users/octavi/Documents/Mendeley Desktop/Arindra et al. - 2017 - Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed t.pdf:pdf},
keywords = {computed tomography,computer-aided detection,convolutional networks,deep learning,medical image challenges,pulmonary nodules},
title = {{Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge}},
url = {https://arxiv.org/pdf/1612.08012.pdf},
year = {2017}
}
@article{Haque2014,
abstract = {In this paper, we implemented both sequential and parallel version of fractal image compression algorithms using CUDA (Compute Unified Device Architecture) programming model for parallelizing the program in Graphics Processing Unit for medical images, as they are highly similar within the image itself. There are several improvement in the implementation of the algorithm as well. Fractal image compression is based on the self similarity of an image, meaning an image having similarity in majority of the regions. We take this opportunity to implement the compression algorithm and monitor the effect of it using both parallel and sequential implementation. Fractal compression has the property of high compression rate and the dimensionless scheme. Compression scheme for fractal image is of two kind, one is encoding and another is decoding. Encoding is very much computational expensive. On the other hand decoding is less computational. The application of fractal compression to medical images would allow obtaining much higher compression ratios. While the fractal magnification an inseparable feature of the fractal compression would be very useful in presenting the reconstructed image in a highly readable form. However, like all irreversible methods, the fractal compression is connected with the problem of information loss, which is especially troublesome in the medical imaging. A very time consuming encoding pro- cess, which can last even several hours, is another bothersome drawback of the fractal compression.},
archivePrefix = {arXiv},
arxivId = {1404.0774},
author = {Haque, Md. Enamul and Kaisan, Abdullah Al and Saniat, Mahmudur R and Rahman, Aminur},
eprint = {1404.0774},
file = {:Users/octavi/Documents/Mendeley Desktop/1404.0774.pdf:pdf},
pages = {1--7},
title = {{GPU Accelerated Fractal Image Compression for Medical Imaging in Parallel Computing Platform}},
url = {http://arxiv.org/abs/1404.0774v1},
year = {2014}
}
@article{Smith2015,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
doi = {10.1109/WACV.2017.58},
eprint = {1506.01186},
file = {:Users/octavi/Documents/Mendeley Desktop/1506.01186.pdf:pdf},
isbn = {9781509048229},
number = {April},
title = {{Cyclical Learning Rates for Training Neural Networks}},
url = {http://arxiv.org/abs/1506.01186},
year = {2015}
}
@article{Postel,
author = {Postel, J.},
title = {{Transmission Control Protocol}},
url = {https://tools.ietf.org/html/rfc793},
year = {1981}
}
@article{Sculley2014,
abstract = {Machine learning offers a fantastically powerful toolkit for building complex sys-tems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is re-markably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several ma-chine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/octavi/Documents/Mendeley Desktop/43146.pdf:pdf},
isbn = {9780874216561},
issn = {13613723},
journal = {NIPS 2014 Workshop on Software Engineering for Machine Learning (SE4ML)},
pages = {1--9},
pmid = {1000106311},
title = {{Machine Learning : The High-Interest Credit Card of Technical Debt}},
year = {2014}
}
